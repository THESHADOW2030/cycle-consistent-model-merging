digraph {
	graph [size="75.3,75.3"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	130775405338848 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	130775788681152 [label=LogSoftmaxBackward0]
	130775788680240 -> 130775788681152
	130775788680240 [label=AddmmBackward0]
	130775788681536 -> 130775788680240
	130775598993792 [label="model.linear.bias
 (10)" fillcolor=lightblue]
	130775598993792 -> 130775788681536
	130775788681536 [label=AccumulateGrad]
	130775788680912 -> 130775788680240
	130775788680912 [label=MeanBackward1]
	130775788679328 -> 130775788680912
	130775788679328 [label=ReluBackward0]
	130775788681296 -> 130775788679328
	130775788681296 [label=AddBackward0]
	130775788681920 -> 130775788681296
	130775788681920 [label=PermuteBackward0]
	130775788683072 -> 130775788681920
	130775788683072 [label=NativeLayerNormBackward0]
	130775788680048 -> 130775788683072
	130775788680048 [label=PermuteBackward0]
	130775788680480 -> 130775788680048
	130775788680480 [label=ConvolutionBackward0]
	130775788681248 -> 130775788680480
	130775788681248 [label=ReluBackward0]
	130775601256720 -> 130775788681248
	130775601256720 [label=PermuteBackward0]
	130775601256768 -> 130775601256720
	130775601256768 [label=NativeLayerNormBackward0]
	130775600088352 -> 130775601256768
	130775600088352 [label=PermuteBackward0]
	130775600088304 -> 130775600088352
	130775600088304 [label=ConvolutionBackward0]
	130775788679856 -> 130775600088304
	130775788679856 [label=ReluBackward0]
	130775600087248 -> 130775788679856
	130775600087248 [label=AddBackward0]
	130775600088640 -> 130775600087248
	130775600088640 [label=PermuteBackward0]
	130775600087152 -> 130775600088640
	130775600087152 [label=NativeLayerNormBackward0]
	130775600087104 -> 130775600087152
	130775600087104 [label=PermuteBackward0]
	130775405584192 -> 130775600087104
	130775405584192 [label=ConvolutionBackward0]
	130775406135520 -> 130775405584192
	130775406135520 [label=ReluBackward0]
	130775406467584 -> 130775406135520
	130775406467584 [label=PermuteBackward0]
	130775405684816 -> 130775406467584
	130775405684816 [label=NativeLayerNormBackward0]
	130775406311504 -> 130775405684816
	130775406311504 [label=PermuteBackward0]
	130775601344864 -> 130775406311504
	130775601344864 [label=ConvolutionBackward0]
	130775600087632 -> 130775601344864
	130775600087632 [label=ReluBackward0]
	130775601345920 -> 130775600087632
	130775601345920 [label=AddBackward0]
	130775601345296 -> 130775601345920
	130775601345296 [label=PermuteBackward0]
	130775601345344 -> 130775601345296
	130775601345344 [label=NativeLayerNormBackward0]
	130775601344960 -> 130775601345344
	130775601344960 [label=PermuteBackward0]
	130775601345152 -> 130775601344960
	130775601345152 [label=ConvolutionBackward0]
	130775601345728 -> 130775601345152
	130775601345728 [label=ReluBackward0]
	130775601345632 -> 130775601345728
	130775601345632 [label=PermuteBackward0]
	130775601344624 -> 130775601345632
	130775601344624 [label=NativeLayerNormBackward0]
	130775601346016 -> 130775601344624
	130775601346016 [label=PermuteBackward0]
	130775601346208 -> 130775601346016
	130775601346208 [label=ConvolutionBackward0]
	130775601346304 -> 130775601346208
	130775601346304 [label=ReluBackward0]
	130775601346448 -> 130775601346304
	130775601346448 [label=AddBackward0]
	130775601346544 -> 130775601346448
	130775601346544 [label=PermuteBackward0]
	130775601346688 -> 130775601346544
	130775601346688 [label=NativeLayerNormBackward0]
	130775601346784 -> 130775601346688
	130775601346784 [label=PermuteBackward0]
	130775601346976 -> 130775601346784
	130775601346976 [label=ConvolutionBackward0]
	130775601347072 -> 130775601346976
	130775601347072 [label=ReluBackward0]
	130775601347216 -> 130775601347072
	130775601347216 [label=PermuteBackward0]
	130775601347312 -> 130775601347216
	130775601347312 [label=NativeLayerNormBackward0]
	130775601347408 -> 130775601347312
	130775601347408 [label=PermuteBackward0]
	130775601347600 -> 130775601347408
	130775601347600 [label=ConvolutionBackward0]
	130775601346496 -> 130775601347600
	130775601346496 [label=ReluBackward0]
	130775601347792 -> 130775601346496
	130775601347792 [label=AddBackward0]
	130775601347888 -> 130775601347792
	130775601347888 [label=PermuteBackward0]
	130775601348032 -> 130775601347888
	130775601348032 [label=NativeLayerNormBackward0]
	130775601348128 -> 130775601348032
	130775601348128 [label=PermuteBackward0]
	130775601348320 -> 130775601348128
	130775601348320 [label=ConvolutionBackward0]
	130775601348416 -> 130775601348320
	130775601348416 [label=ReluBackward0]
	130775601348560 -> 130775601348416
	130775601348560 [label=PermuteBackward0]
	130775601348464 -> 130775601348560
	130775601348464 [label=NativeLayerNormBackward0]
	130775601148112 -> 130775601348464
	130775601148112 [label=PermuteBackward0]
	130775601148304 -> 130775601148112
	130775601148304 [label=ConvolutionBackward0]
	130775601347840 -> 130775601148304
	130775601347840 [label=ReluBackward0]
	130775601148496 -> 130775601347840
	130775601148496 [label=AddBackward0]
	130775601148592 -> 130775601148496
	130775601148592 [label=PermuteBackward0]
	130775601148736 -> 130775601148592
	130775601148736 [label=NativeLayerNormBackward0]
	130775601148832 -> 130775601148736
	130775601148832 [label=PermuteBackward0]
	130775601149024 -> 130775601148832
	130775601149024 [label=ConvolutionBackward0]
	130775601149120 -> 130775601149024
	130775601149120 [label=ReluBackward0]
	130775601149264 -> 130775601149120
	130775601149264 [label=PermuteBackward0]
	130775601149360 -> 130775601149264
	130775601149360 [label=NativeLayerNormBackward0]
	130775601149456 -> 130775601149360
	130775601149456 [label=PermuteBackward0]
	130775601149648 -> 130775601149456
	130775601149648 [label=ConvolutionBackward0]
	130775601149744 -> 130775601149648
	130775601149744 [label=ReluBackward0]
	130775601149888 -> 130775601149744
	130775601149888 [label=AddBackward0]
	130775601149984 -> 130775601149888
	130775601149984 [label=PermuteBackward0]
	130775601150128 -> 130775601149984
	130775601150128 [label=NativeLayerNormBackward0]
	130775601150224 -> 130775601150128
	130775601150224 [label=PermuteBackward0]
	130775601150416 -> 130775601150224
	130775601150416 [label=ConvolutionBackward0]
	130775601150512 -> 130775601150416
	130775601150512 [label=ReluBackward0]
	130775601150656 -> 130775601150512
	130775601150656 [label=PermuteBackward0]
	130775601150752 -> 130775601150656
	130775601150752 [label=NativeLayerNormBackward0]
	130775601150848 -> 130775601150752
	130775601150848 [label=PermuteBackward0]
	130775788647184 -> 130775601150848
	130775788647184 [label=ConvolutionBackward0]
	130775601149936 -> 130775788647184
	130775601149936 [label=ReluBackward0]
	130775788693392 -> 130775601149936
	130775788693392 [label=AddBackward0]
	130775788693056 -> 130775788693392
	130775788693056 [label=PermuteBackward0]
	130775601151040 -> 130775788693056
	130775601151040 [label=NativeLayerNormBackward0]
	130775601151136 -> 130775601151040
	130775601151136 [label=PermuteBackward0]
	130775601151328 -> 130775601151136
	130775601151328 [label=ConvolutionBackward0]
	130775601151424 -> 130775601151328
	130775601151424 [label=ReluBackward0]
	130775601151568 -> 130775601151424
	130775601151568 [label=PermuteBackward0]
	130775601151664 -> 130775601151568
	130775601151664 [label=NativeLayerNormBackward0]
	130775601151760 -> 130775601151664
	130775601151760 [label=PermuteBackward0]
	130775601151952 -> 130775601151760
	130775601151952 [label=ConvolutionBackward0]
	130775788694208 -> 130775601151952
	130775788694208 [label=ReluBackward0]
	130775601082176 -> 130775788694208
	130775601082176 [label=AddBackward0]
	130775601082080 -> 130775601082176
	130775601082080 [label=PermuteBackward0]
	130775601081936 -> 130775601082080
	130775601081936 [label=NativeLayerNormBackward0]
	130775601081840 -> 130775601081936
	130775601081840 [label=PermuteBackward0]
	130775601081648 -> 130775601081840
	130775601081648 [label=ConvolutionBackward0]
	130775601081552 -> 130775601081648
	130775601081552 [label=ReluBackward0]
	130775601081408 -> 130775601081552
	130775601081408 [label=PermuteBackward0]
	130775601081312 -> 130775601081408
	130775601081312 [label=NativeLayerNormBackward0]
	130775601081216 -> 130775601081312
	130775601081216 [label=PermuteBackward0]
	130775601081024 -> 130775601081216
	130775601081024 [label=ConvolutionBackward0]
	130775601082128 -> 130775601081024
	130775601082128 [label=ReluBackward0]
	130775601080832 -> 130775601082128
	130775601080832 [label=PermuteBackward0]
	130775601080736 -> 130775601080832
	130775601080736 [label=NativeLayerNormBackward0]
	130775601080640 -> 130775601080736
	130775601080640 [label=PermuteBackward0]
	130775601080448 -> 130775601080640
	130775601080448 [label=ConvolutionBackward0]
	130775601080352 -> 130775601080448
	130775599174816 [label="model.conv1.weight
 (32, 3, 3, 3)" fillcolor=lightblue]
	130775599174816 -> 130775601080352
	130775601080352 [label=AccumulateGrad]
	130775601080688 -> 130775601080736
	130775599175376 [label="model.bn1.layer_norm.weight
 (32)" fillcolor=lightblue]
	130775599175376 -> 130775601080688
	130775601080688 [label=AccumulateGrad]
	130775601081120 -> 130775601080736
	130775599175456 [label="model.bn1.layer_norm.bias
 (32)" fillcolor=lightblue]
	130775599175456 -> 130775601081120
	130775601081120 [label=AccumulateGrad]
	130775601080928 -> 130775601081024
	130775599175616 [label="model.blockgroup1.block1.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	130775599175616 -> 130775601080928
	130775601080928 [label=AccumulateGrad]
	130775601081264 -> 130775601081312
	130775599175696 [label="model.blockgroup1.block1.bn1.layer_norm.weight
 (32)" fillcolor=lightblue]
	130775599175696 -> 130775601081264
	130775601081264 [label=AccumulateGrad]
	130775601081504 -> 130775601081312
	130775599175776 [label="model.blockgroup1.block1.bn1.layer_norm.bias
 (32)" fillcolor=lightblue]
	130775599175776 -> 130775601081504
	130775601081504 [label=AccumulateGrad]
	130775601081600 -> 130775601081648
	130775599175936 [label="model.blockgroup1.block1.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	130775599175936 -> 130775601081600
	130775601081600 [label=AccumulateGrad]
	130775601081888 -> 130775601081936
	130775599176016 [label="model.blockgroup1.block1.bn2.layer_norm.weight
 (32)" fillcolor=lightblue]
	130775599176016 -> 130775601081888
	130775601081888 [label=AccumulateGrad]
	130775601082032 -> 130775601081936
	130775599176096 [label="model.blockgroup1.block1.bn2.layer_norm.bias
 (32)" fillcolor=lightblue]
	130775599176096 -> 130775601082032
	130775601082032 [label=AccumulateGrad]
	130775601082128 -> 130775601082176
	130775601151856 -> 130775601151952
	130775599176256 [label="model.blockgroup1.block2.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	130775599176256 -> 130775601151856
	130775601151856 [label=AccumulateGrad]
	130775601151712 -> 130775601151664
	130775599176176 [label="model.blockgroup1.block2.bn1.layer_norm.weight
 (32)" fillcolor=lightblue]
	130775599176176 -> 130775601151712
	130775601151712 [label=AccumulateGrad]
	130775601151472 -> 130775601151664
	130775599176416 [label="model.blockgroup1.block2.bn1.layer_norm.bias
 (32)" fillcolor=lightblue]
	130775599176416 -> 130775601151472
	130775601151472 [label=AccumulateGrad]
	130775601151376 -> 130775601151328
	130775599176576 [label="model.blockgroup1.block2.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	130775599176576 -> 130775601151376
	130775601151376 [label=AccumulateGrad]
	130775601151088 -> 130775601151040
	130775599176656 [label="model.blockgroup1.block2.bn2.layer_norm.weight
 (32)" fillcolor=lightblue]
	130775599176656 -> 130775601151088
	130775601151088 [label=AccumulateGrad]
	130775601150992 -> 130775601151040
	130775599176736 [label="model.blockgroup1.block2.bn2.layer_norm.bias
 (32)" fillcolor=lightblue]
	130775599176736 -> 130775601150992
	130775601150992 [label=AccumulateGrad]
	130775788694208 -> 130775788693392
	130775788694256 -> 130775788647184
	130775599176896 [label="model.blockgroup1.block3.conv1.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	130775599176896 -> 130775788694256
	130775788694256 [label=AccumulateGrad]
	130775601150800 -> 130775601150752
	130775599176976 [label="model.blockgroup1.block3.bn1.layer_norm.weight
 (32)" fillcolor=lightblue]
	130775599176976 -> 130775601150800
	130775601150800 [label=AccumulateGrad]
	130775601150560 -> 130775601150752
	130775599177056 [label="model.blockgroup1.block3.bn1.layer_norm.bias
 (32)" fillcolor=lightblue]
	130775599177056 -> 130775601150560
	130775601150560 [label=AccumulateGrad]
	130775601150464 -> 130775601150416
	130775599177216 [label="model.blockgroup1.block3.conv2.weight
 (32, 32, 3, 3)" fillcolor=lightblue]
	130775599177216 -> 130775601150464
	130775601150464 [label=AccumulateGrad]
	130775601150176 -> 130775601150128
	130775599177296 [label="model.blockgroup1.block3.bn2.layer_norm.weight
 (32)" fillcolor=lightblue]
	130775599177296 -> 130775601150176
	130775601150176 [label=AccumulateGrad]
	130775601150032 -> 130775601150128
	130775599177376 [label="model.blockgroup1.block3.bn2.layer_norm.bias
 (32)" fillcolor=lightblue]
	130775599177376 -> 130775601150032
	130775601150032 [label=AccumulateGrad]
	130775601149936 -> 130775601149888
	130775601149696 -> 130775601149648
	130775599177536 [label="model.blockgroup2.block1.conv1.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	130775599177536 -> 130775601149696
	130775601149696 [label=AccumulateGrad]
	130775601149408 -> 130775601149360
	130775599177616 [label="model.blockgroup2.block1.bn1.layer_norm.weight
 (64)" fillcolor=lightblue]
	130775599177616 -> 130775601149408
	130775601149408 [label=AccumulateGrad]
	130775601149168 -> 130775601149360
	130775598805056 [label="model.blockgroup2.block1.bn1.layer_norm.bias
 (64)" fillcolor=lightblue]
	130775598805056 -> 130775601149168
	130775601149168 [label=AccumulateGrad]
	130775601149072 -> 130775601149024
	130775598805216 [label="model.blockgroup2.block1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	130775598805216 -> 130775601149072
	130775601149072 [label=AccumulateGrad]
	130775601148784 -> 130775601148736
	130775598805296 [label="model.blockgroup2.block1.bn2.layer_norm.weight
 (64)" fillcolor=lightblue]
	130775598805296 -> 130775601148784
	130775601148784 [label=AccumulateGrad]
	130775601148640 -> 130775601148736
	130775598805376 [label="model.blockgroup2.block1.bn2.layer_norm.bias
 (64)" fillcolor=lightblue]
	130775598805376 -> 130775601148640
	130775601148640 [label=AccumulateGrad]
	130775601148544 -> 130775601148496
	130775601148544 [label=PermuteBackward0]
	130775788694688 -> 130775601148544
	130775788694688 [label=NativeLayerNormBackward0]
	130775788692720 -> 130775788694688
	130775788692720 [label=PermuteBackward0]
	130775601149216 -> 130775788692720
	130775601149216 [label=ConvolutionBackward0]
	130775601149744 -> 130775601149216
	130775601149504 -> 130775601149216
	130775598805536 [label="model.blockgroup2.block1.shortcut.0.weight
 (64, 32, 3, 3)" fillcolor=lightblue]
	130775598805536 -> 130775601149504
	130775601149504 [label=AccumulateGrad]
	130775788693152 -> 130775788694688
	130775598805616 [label="model.blockgroup2.block1.shortcut.1.layer_norm.weight
 (64)" fillcolor=lightblue]
	130775598805616 -> 130775788693152
	130775788693152 [label=AccumulateGrad]
	130775601148688 -> 130775788694688
	130775598805696 [label="model.blockgroup2.block1.shortcut.1.layer_norm.bias
 (64)" fillcolor=lightblue]
	130775598805696 -> 130775601148688
	130775601148688 [label=AccumulateGrad]
	130775601148400 -> 130775601148304
	130775598805856 [label="model.blockgroup2.block2.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	130775598805856 -> 130775601148400
	130775601148400 [label=AccumulateGrad]
	130775601148064 -> 130775601348464
	130775598805936 [label="model.blockgroup2.block2.bn1.layer_norm.weight
 (64)" fillcolor=lightblue]
	130775598805936 -> 130775601148064
	130775601148064 [label=AccumulateGrad]
	130775601147968 -> 130775601348464
	130775598806016 [label="model.blockgroup2.block2.bn1.layer_norm.bias
 (64)" fillcolor=lightblue]
	130775598806016 -> 130775601147968
	130775601147968 [label=AccumulateGrad]
	130775601348368 -> 130775601348320
	130775598806176 [label="model.blockgroup2.block2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	130775598806176 -> 130775601348368
	130775601348368 [label=AccumulateGrad]
	130775601348080 -> 130775601348032
	130775598806256 [label="model.blockgroup2.block2.bn2.layer_norm.weight
 (64)" fillcolor=lightblue]
	130775598806256 -> 130775601348080
	130775601348080 [label=AccumulateGrad]
	130775601347936 -> 130775601348032
	130775598806336 [label="model.blockgroup2.block2.bn2.layer_norm.bias
 (64)" fillcolor=lightblue]
	130775598806336 -> 130775601347936
	130775601347936 [label=AccumulateGrad]
	130775601347840 -> 130775601347792
	130775601347696 -> 130775601347600
	130775598806496 [label="model.blockgroup2.block3.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	130775598806496 -> 130775601347696
	130775601347696 [label=AccumulateGrad]
	130775601347360 -> 130775601347312
	130775598806576 [label="model.blockgroup2.block3.bn1.layer_norm.weight
 (64)" fillcolor=lightblue]
	130775598806576 -> 130775601347360
	130775601347360 [label=AccumulateGrad]
	130775601347120 -> 130775601347312
	130775598806656 [label="model.blockgroup2.block3.bn1.layer_norm.bias
 (64)" fillcolor=lightblue]
	130775598806656 -> 130775601347120
	130775601347120 [label=AccumulateGrad]
	130775601347024 -> 130775601346976
	130775598806816 [label="model.blockgroup2.block3.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	130775598806816 -> 130775601347024
	130775601347024 [label=AccumulateGrad]
	130775601346736 -> 130775601346688
	130775598806896 [label="model.blockgroup2.block3.bn2.layer_norm.weight
 (64)" fillcolor=lightblue]
	130775598806896 -> 130775601346736
	130775601346736 [label=AccumulateGrad]
	130775601346592 -> 130775601346688
	130775598806976 [label="model.blockgroup2.block3.bn2.layer_norm.bias
 (64)" fillcolor=lightblue]
	130775598806976 -> 130775601346592
	130775601346592 [label=AccumulateGrad]
	130775601346496 -> 130775601346448
	130775601346256 -> 130775601346208
	130775598807136 [label="model.blockgroup3.block1.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	130775598807136 -> 130775601346256
	130775601346256 [label=AccumulateGrad]
	130775601344576 -> 130775601344624
	130775598807216 [label="model.blockgroup3.block1.bn1.layer_norm.weight
 (128)" fillcolor=lightblue]
	130775598807216 -> 130775601344576
	130775601344576 [label=AccumulateGrad]
	130775601345680 -> 130775601344624
	130775598807296 [label="model.blockgroup3.block1.bn1.layer_norm.bias
 (128)" fillcolor=lightblue]
	130775598807296 -> 130775601345680
	130775601345680 [label=AccumulateGrad]
	130775601345776 -> 130775601345152
	130775598807456 [label="model.blockgroup3.block1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	130775598807456 -> 130775601345776
	130775601345776 [label=AccumulateGrad]
	130775601345248 -> 130775601345344
	130775598807536 [label="model.blockgroup3.block1.bn2.layer_norm.weight
 (128)" fillcolor=lightblue]
	130775598807536 -> 130775601345248
	130775601345248 [label=AccumulateGrad]
	130775601345488 -> 130775601345344
	130775598807616 [label="model.blockgroup3.block1.bn2.layer_norm.bias
 (128)" fillcolor=lightblue]
	130775598807616 -> 130775601345488
	130775601345488 [label=AccumulateGrad]
	130775601345968 -> 130775601345920
	130775601345968 [label=PermuteBackward0]
	130775788694448 -> 130775601345968
	130775788694448 [label=NativeLayerNormBackward0]
	130775601344720 -> 130775788694448
	130775601344720 [label=PermuteBackward0]
	130775601346160 -> 130775601344720
	130775601346160 [label=ConvolutionBackward0]
	130775601346304 -> 130775601346160
	130775601346352 -> 130775601346160
	130775598807776 [label="model.blockgroup3.block1.shortcut.0.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	130775598807776 -> 130775601346352
	130775601346352 [label=AccumulateGrad]
	130775601344816 -> 130775788694448
	130775598807856 [label="model.blockgroup3.block1.shortcut.1.layer_norm.weight
 (128)" fillcolor=lightblue]
	130775598807856 -> 130775601344816
	130775601344816 [label=AccumulateGrad]
	130775601345104 -> 130775788694448
	130775598807936 [label="model.blockgroup3.block1.shortcut.1.layer_norm.bias
 (128)" fillcolor=lightblue]
	130775598807936 -> 130775601345104
	130775601345104 [label=AccumulateGrad]
	130775601345824 -> 130775601344864
	130775598808096 [label="model.blockgroup3.block2.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	130775598808096 -> 130775601345824
	130775601345824 [label=AccumulateGrad]
	130775406313232 -> 130775405684816
	130775598808176 [label="model.blockgroup3.block2.bn1.layer_norm.weight
 (128)" fillcolor=lightblue]
	130775598808176 -> 130775406313232
	130775406313232 [label=AccumulateGrad]
	130775406465904 -> 130775405684816
	130775598808256 [label="model.blockgroup3.block2.bn1.layer_norm.bias
 (128)" fillcolor=lightblue]
	130775598808256 -> 130775406465904
	130775406465904 [label=AccumulateGrad]
	130775406135616 -> 130775405584192
	130775598808416 [label="model.blockgroup3.block2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	130775598808416 -> 130775406135616
	130775406135616 [label=AccumulateGrad]
	130775600087872 -> 130775600087152
	130775598808496 [label="model.blockgroup3.block2.bn2.layer_norm.weight
 (128)" fillcolor=lightblue]
	130775598808496 -> 130775600087872
	130775600087872 [label=AccumulateGrad]
	130775600087488 -> 130775600087152
	130775598808576 [label="model.blockgroup3.block2.bn2.layer_norm.bias
 (128)" fillcolor=lightblue]
	130775598808576 -> 130775600087488
	130775600087488 [label=AccumulateGrad]
	130775600087632 -> 130775600087248
	130775600087344 -> 130775600088304
	130775598808736 [label="model.blockgroup3.block3.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	130775598808736 -> 130775600087344
	130775600087344 [label=AccumulateGrad]
	130775600088544 -> 130775601256768
	130775598808816 [label="model.blockgroup3.block3.bn1.layer_norm.weight
 (128)" fillcolor=lightblue]
	130775598808816 -> 130775600088544
	130775600088544 [label=AccumulateGrad]
	130775600088112 -> 130775601256768
	130775598808896 [label="model.blockgroup3.block3.bn1.layer_norm.bias
 (128)" fillcolor=lightblue]
	130775598808896 -> 130775600088112
	130775600088112 [label=AccumulateGrad]
	130775788679952 -> 130775788680480
	130775598993472 [label="model.blockgroup3.block3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	130775598993472 -> 130775788679952
	130775788679952 [label=AccumulateGrad]
	130775788680384 -> 130775788683072
	130775598993552 [label="model.blockgroup3.block3.bn2.layer_norm.weight
 (128)" fillcolor=lightblue]
	130775598993552 -> 130775788680384
	130775788680384 [label=AccumulateGrad]
	130775788683120 -> 130775788683072
	130775598993632 [label="model.blockgroup3.block3.bn2.layer_norm.bias
 (128)" fillcolor=lightblue]
	130775598993632 -> 130775788683120
	130775788683120 [label=AccumulateGrad]
	130775788679856 -> 130775788681296
	130775788680432 -> 130775788680240
	130775788680432 [label=TBackward0]
	130775788682736 -> 130775788680432
	130775598993712 [label="model.linear.weight
 (10, 128)" fillcolor=lightblue]
	130775598993712 -> 130775788682736
	130775788682736 [label=AccumulateGrad]
	130775788681152 -> 130775405338848
}
