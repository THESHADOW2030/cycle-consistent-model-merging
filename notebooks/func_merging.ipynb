{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import itertools\n",
    "import logging\n",
    "import math\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import json\n",
    "\n",
    "import hydra\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import omegaconf\n",
    "import pytorch_lightning\n",
    "import seaborn as sns\n",
    "import torch  # noqa\n",
    "import wandb\n",
    "from hydra.utils import instantiate\n",
    "from matplotlib import tri\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning import LightningModule\n",
    "from scipy.stats import qmc\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nn_core.callbacks import NNTemplateCore\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "from nn_core.common.utils import seed_index_everything\n",
    "from nn_core.model_logging import NNLogger\n",
    "\n",
    "import ccmm  # noqa\n",
    "from ccmm.matching.utils import (\n",
    "    apply_permutation_to_statedict,\n",
    "    get_all_symbols_combinations,\n",
    "    load_permutations,\n",
    "    perm_indices_to_perm_matrix,\n",
    "    plot_permutation_history_animation,\n",
    "    restore_original_weights,\n",
    ")\n",
    "from ccmm.utils.utils import (\n",
    "    fuse_batch_norm_into_conv,\n",
    "    get_interpolated_loss_acc_curves,\n",
    "    l2_norm_models,\n",
    "    linear_interpolate,\n",
    "    load_model_from_info,\n",
    "    map_model_seed_to_symbol,\n",
    "    normalize_unit_norm,\n",
    "    project_onto,\n",
    "    save_factored_permutations,\n",
    "    vector_to_state_dict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update(\n",
    "    {\n",
    "        \"text.usetex\": True,\n",
    "        \"font.family\": \"serif\",\n",
    "    }\n",
    ")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "cmap_name = \"coolwarm_r\"\n",
    "\n",
    "from ccmm.utils.plot import Palette\n",
    "\n",
    "palette = Palette(f\"{PROJECT_ROOT}/misc/palette2.json\")\n",
    "palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").setLevel(logging.WARNING)\n",
    "pylogger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from typing import Dict, List\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=str(\"../conf\"), job_name=\"matching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"matching\", overrides=[\"model=mlp\", \"dataset=emnist\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_cfg = cfg  # NOQA\n",
    "cfg = cfg.matching\n",
    "\n",
    "seed_index_everything(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 5000\n",
    "num_train_samples = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = instantiate(core_cfg.dataset.test.transform)\n",
    "\n",
    "train_dataset = instantiate(core_cfg.dataset.train, transform=transform)\n",
    "test_dataset = instantiate(core_cfg.dataset.test, transform=transform)\n",
    "\n",
    "train_subset = Subset(train_dataset, list(range(num_train_samples)))\n",
    "train_loader = DataLoader(train_subset, batch_size=5000, num_workers=cfg.num_workers)\n",
    "\n",
    "test_subset = Subset(test_dataset, list(range(num_test_samples)))\n",
    "\n",
    "test_loader = DataLoader(test_subset, batch_size=1000, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = instantiate(cfg.trainer, enable_progress_bar=False, enable_model_summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.utils.utils import load_model_from_artifact\n",
    "\n",
    "run = wandb.init(\n",
    "    project=core_cfg.core.project_name, entity=\"theshadow2030-sapienza-universit-di-roma\", job_type=\"matching\"\n",
    ")\n",
    "\n",
    "# {a: 1, b: 2, c: 3, ..}\n",
    "symbols_to_seed: Dict[int, str] = {map_model_seed_to_symbol(seed): seed for seed in cfg.model_seeds}\n",
    "\n",
    "artifact_path = (\n",
    "    lambda seed: f\"{core_cfg.core.entity}/{core_cfg.core.project_name}/{core_cfg.dataset.name}_{core_cfg.model.model_identifier}_{seed}:latest\"\n",
    ")\n",
    "\n",
    "# {a: model_a, b: model_b, c: model_c, ..}\n",
    "models: Dict[str, LightningModule] = {\n",
    "    map_model_seed_to_symbol(seed): load_model_from_artifact(run, artifact_path(seed)) for seed in cfg.model_seeds\n",
    "}\n",
    "model_orig_weights = {symbol: copy.deepcopy(model.model.state_dict()) for symbol, model in models.items()}\n",
    "\n",
    "num_models = len(models)\n",
    "\n",
    "pylogger.info(f\"Using {num_models} models with architecture {core_cfg.model.model_identifier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always permute the model having larger character order, i.e. c -> b, b -> a and so on ...\n",
    "from ccmm.matching.matcher import GitRebasinMatcher\n",
    "from ccmm.matching.utils import get_inverse_permutations\n",
    "\n",
    "symbols = set(symbols_to_seed.keys())\n",
    "sorted_symbols = sorted(symbols, reverse=False)\n",
    "fixed_symbol, permutee_symbol = \"a\", \"b\"\n",
    "fixed_model, permutee_model = models[fixed_symbol].cpu(), models[permutee_symbol].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.matching.permutation_spec import MLPPermutationSpecBuilder, CNNPermutationSpecBuilder\n",
    "\n",
    "permutation_spec_builder = MLPPermutationSpecBuilder(num_hidden_layers=4)\n",
    "# permutation_spec_builder = CNNPermutationSpecBuilder()\n",
    "permutation_spec = permutation_spec_builder.create_permutation_spec()\n",
    "\n",
    "ref_model = list(models.values())[0]\n",
    "assert set(permutation_spec.layer_and_axes_to_perm.keys()) == set(ref_model.model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Func maps Weight matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_original_weights(models, model_orig_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.matching.func_maps import FM_to_p2p, graph_zoomout_refine\n",
    "\n",
    "w_descrs = 100\n",
    "w_laps = 10\n",
    "w_dcomms = 0.5\n",
    "\n",
    "num_neighbors = 20\n",
    "\n",
    "InitFM_mode = \"identity\"\n",
    "\n",
    "num_zoomout_iters = 70\n",
    "step = 1\n",
    "opt_descriptor_type = \"weights\"  # \"weights\", \"features\", \"features_denoised\", \"eigenneurons\", \"spectral\"\n",
    "mode = \"connectivity\"  # connectivity, distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicts for permutations and permuted params, D[a][b] refers to the permutation/params to map b -> a\n",
    "func_permutations = {symb: {other_symb: None for other_symb in symbols.difference(symb)} for symb in symbols}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from ccmm.matching.func_maps import compute_eigenvectors, fit_func_map\n",
    "from ccmm.matching.permutation_spec import PermutationSpec\n",
    "from ccmm.matching.utils import get_permuted_param, perm_cols, perm_matrix_to_perm_indices, perm_rows\n",
    "from ccmm.matching.weight_matching import (\n",
    "    LayerIterationOrder,\n",
    "    compute_weights_similarity,\n",
    "    get_layer_iteration_order,\n",
    "    solve_linear_assignment_problem,\n",
    ")\n",
    "\n",
    "\n",
    "def func_weight_matching(\n",
    "    ps: PermutationSpec,\n",
    "    fixed,\n",
    "    permutee,\n",
    "    max_iter=100,\n",
    "    init_perm=None,\n",
    "    layer_iteration_order: LayerIterationOrder = LayerIterationOrder.RANDOM,\n",
    "    verbose=False,\n",
    "    method=\"func\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Find a permutation of params_b to make them match params_a.\n",
    "\n",
    "    :param ps: PermutationSpec\n",
    "    :param target: the parameters to match\n",
    "    :param to_permute: the parameters to permute\n",
    "    \"\"\"\n",
    "\n",
    "    if not verbose:\n",
    "        pylogger.setLevel(logging.WARNING)\n",
    "\n",
    "    params_a, params_b = fixed, permutee\n",
    "\n",
    "    perm_sizes = {}\n",
    "\n",
    "    for p, params_and_axes in ps.perm_to_layers_and_axes.items():\n",
    "\n",
    "        # p is the permutation matrix name, e.g. P_0, P_1, ..\n",
    "        # params_and_axes is a list of tuples, each tuple contains the name of the parameter and the axis on which the permutation matrix acts\n",
    "\n",
    "        # it is enough to select a single parameter and axis, since all the parameters permuted by the same matrix have the same shape\n",
    "        ref_tuple = params_and_axes[0]\n",
    "        ref_param_name = ref_tuple[0]\n",
    "        ref_axis = ref_tuple[1]\n",
    "\n",
    "        perm_sizes[p] = params_a[ref_param_name].shape[ref_axis]\n",
    "\n",
    "    # initialize with identity permutation if none given\n",
    "    all_perm_indices = {p: torch.arange(n) for p, n in perm_sizes.items()} if init_perm is None else init_perm\n",
    "    # e.g. P0, P1, ..\n",
    "    perm_names = list(all_perm_indices.keys())\n",
    "\n",
    "    num_layers = len(perm_names)\n",
    "\n",
    "    for iteration in tqdm(range(max_iter), desc=\"Weight matching\"):\n",
    "        progress = False\n",
    "\n",
    "        perm_order = get_layer_iteration_order(layer_iteration_order, num_layers)\n",
    "\n",
    "        for p_ix in perm_order:\n",
    "            print(f\"Permuting {perm_names[p_ix]}\")\n",
    "\n",
    "            p = perm_names[p_ix]\n",
    "            num_neurons = perm_sizes[p]\n",
    "\n",
    "            # all the params that are permuted by this permutation matrix, together with the axis on which it acts\n",
    "            # e.g. ('layer_0.weight', 0), ('layer_0.bias', 0), ('layer_1.weight', 0)..\n",
    "            params_and_axes: List[Tuple[str, int]] = ps.perm_to_layers_and_axes[p]\n",
    "            # sort by axis, so that we can permute the columns of the weight matrices first\n",
    "            params_and_axes = sorted(params_and_axes, key=lambda x: x[1])\n",
    "            # filter out bias\n",
    "            params_and_axes = [x for x in params_and_axes if \"bias\" not in x[0]]\n",
    "\n",
    "            # TODO: check if this is true in more complex architectures (probably not)\n",
    "            assert len(params_and_axes) == 2, f\"Expected 2 params, got {len(params_and_axes)}: {params_and_axes}\"\n",
    "\n",
    "            # AXES 0, CURRENT LAYER\n",
    "            curr_params_name = params_and_axes[0][0]\n",
    "            assert params_and_axes[0][1] == 0\n",
    "\n",
    "            # (num_neurons, neuron_dim)\n",
    "            w_a = copy.deepcopy(params_a[curr_params_name])\n",
    "            w_b = copy.deepcopy(params_b[curr_params_name])\n",
    "            assert w_a.shape == w_b.shape\n",
    "\n",
    "            perms_to_apply = ps.layer_and_axes_to_perm[curr_params_name]\n",
    "\n",
    "            col_perm_to_apply = perms_to_apply[1]\n",
    "\n",
    "            if col_perm_to_apply is not None:\n",
    "                # apply the tranpose of the previous permutation to the columns of the current layer\n",
    "                perm_matrix = perm_indices_to_perm_matrix(all_perm_indices[col_perm_to_apply])\n",
    "                print(\n",
    "                    f\"Permuting the columns of {curr_params_name} with {col_perm_to_apply}, shape: {perm_matrix.shape}\"\n",
    "                )\n",
    "\n",
    "                w_b = perm_cols(w_b, perm_matrix.T)\n",
    "\n",
    "            # AXES 1, NEXT LAYER\n",
    "            next_params_name = params_and_axes[1][0]\n",
    "            w_a_next = copy.deepcopy(params_a[next_params_name])\n",
    "            w_b_next = copy.deepcopy(params_b[next_params_name])\n",
    "\n",
    "            assert w_a_next.shape == w_b_next.shape\n",
    "\n",
    "            perms_to_apply = ps.layer_and_axes_to_perm[next_params_name]\n",
    "            perm_row_next_layer = perms_to_apply[0]\n",
    "\n",
    "            if perm_row_next_layer is not None:\n",
    "                # permute the rows of the next layer by its permutation matrix\n",
    "                w_b_next = perm_rows(w_b_next, perm_indices_to_perm_matrix(all_perm_indices[perm_row_next_layer]))\n",
    "\n",
    "            w_a_next = torch.moveaxis(w_a_next, 1, 0).reshape((num_neurons, -1))\n",
    "            w_b_next = torch.moveaxis(w_b_next, 1, 0).reshape((num_neurons, -1))\n",
    "\n",
    "            w_a = torch.cat((w_a, w_a_next), dim=1)\n",
    "            w_b = torch.cat((w_b, w_b_next), dim=1)\n",
    "\n",
    "            print(f\"w_a shape: {w_a.shape}, w_b shape: {w_b.shape}\")\n",
    "\n",
    "            sim_matrix = w_a @ w_b.T\n",
    "\n",
    "            if method == \"func\":\n",
    "\n",
    "                w_b_evecs, w_a_evecs, w_b_evals, w_a_evals = compute_eigenvectors(\n",
    "                    w_b.numpy(), w_a.numpy(), radius=None, num_neighbors=num_neighbors, mode=mode, normalize_lap=True\n",
    "                )\n",
    "\n",
    "                k1, k2 = int(0.5 * num_neurons), int(0.5 * num_neurons)\n",
    "                print(f\"k1: {k1}, k2: {k2}\")\n",
    "\n",
    "                FM_opt, FM_loss = fit_func_map(\n",
    "                    w_b.numpy(),\n",
    "                    w_a.numpy(),\n",
    "                    w_b_evecs,\n",
    "                    w_a_evecs,\n",
    "                    w_b_evals,\n",
    "                    w_a_evals,\n",
    "                    k1,\n",
    "                    k2,\n",
    "                    InitFM_mode,\n",
    "                    w_descrs,\n",
    "                    w_lap=w_laps,\n",
    "                    w_dcomm=w_dcomms,\n",
    "                    method=\"optimize\",\n",
    "                )\n",
    "\n",
    "                FM_opt_zo = graph_zoomout_refine(FM_opt, w_b_evecs, w_a_evecs, num_iters=num_zoomout_iters, step=step)\n",
    "\n",
    "                perm = FM_to_p2p(FM_opt_zo, w_b_evecs, w_a_evecs, n_jobs=1)\n",
    "\n",
    "            elif method == \"lap\":\n",
    "                perm = solve_linear_assignment_problem(sim_matrix, return_matrix=True)\n",
    "\n",
    "            # plt.imshow(perm, cmap=\"coolwarm\")\n",
    "            # plt.show()\n",
    "\n",
    "            old_similarity = compute_weights_similarity(sim_matrix, all_perm_indices[p])\n",
    "\n",
    "            all_perm_indices[p] = perm_matrix_to_perm_indices(perm)\n",
    "\n",
    "            new_similarity = compute_weights_similarity(sim_matrix, all_perm_indices[p])\n",
    "\n",
    "            pylogger.info(f\"Iteration {iteration}, Permutation {p}: {new_similarity - old_similarity}\")\n",
    "\n",
    "            progress = progress or new_similarity > old_similarity + 1e-12\n",
    "            # loss_decrease = loss[p] - FM_loss\n",
    "            # progress = progress or loss_decrease > 1e-6\n",
    "            # pylogger.info(f\"Loss decrease for {p}: {loss_decrease}\")\n",
    "\n",
    "            # loss[p] = FM_loss\n",
    "\n",
    "        if not progress:\n",
    "            break\n",
    "\n",
    "    return all_perm_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_original_weights(models, model_orig_weights)\n",
    "\n",
    "perm_indices = func_weight_matching(\n",
    "    permutation_spec,\n",
    "    fixed_model.model.cpu().state_dict(),\n",
    "    permutee_model.model.cpu().state_dict(),\n",
    "    max_iter=100,\n",
    "    verbose=True,\n",
    "    method=\"func\",\n",
    ")\n",
    "\n",
    "func_permutations[fixed_symbol][permutee_symbol] = perm_indices\n",
    "\n",
    "func_permutations[permutee_symbol][fixed_symbol] = get_inverse_permutations(perm_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.evaluate_matched_models import evaluate_pair_of_models\n",
    "\n",
    "restore_original_weights(models, model_orig_weights)\n",
    "\n",
    "updated_params = {fixed_symbol: {permutee_symbol: None}}\n",
    "\n",
    "pylogger.info(f\"Permuting model {permutee_symbol} into {fixed_symbol}.\")\n",
    "\n",
    "# perms[a, b] maps b -> a\n",
    "updated_params[fixed_symbol][permutee_symbol] = apply_permutation_to_statedict(\n",
    "    permutation_spec, func_permutations[fixed_symbol][permutee_symbol], models[permutee_symbol].model.state_dict()\n",
    ")\n",
    "restore_original_weights(models, model_orig_weights)\n",
    "\n",
    "lambdas = [0.0, 0.5, 1.0]\n",
    "\n",
    "func_results = evaluate_pair_of_models(\n",
    "    models,\n",
    "    fixed_symbol,\n",
    "    permutee_symbol,\n",
    "    updated_params,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    lambdas,\n",
    "    core_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_original_weights(models, model_orig_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.matching.matcher import DummyMatcher\n",
    "\n",
    "matcher = DummyMatcher(name=\"naive\", permutation_spec=permutation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicts for permutations and permuted params, D[a][b] refers to the permutation/params to map b -> a\n",
    "naive_permutations = {symb: {other_symb: None for other_symb in symbols.difference(symb)} for symb in symbols}\n",
    "\n",
    "naive_permutations[fixed_symbol][permutee_symbol], perm_history = matcher(\n",
    "    fixed=fixed_model.model, permutee=permutee_model.model\n",
    ")\n",
    "\n",
    "naive_permutations[permutee_symbol][fixed_symbol] = get_inverse_permutations(\n",
    "    naive_permutations[fixed_symbol][permutee_symbol]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.evaluate_matched_models import evaluate_pair_of_models\n",
    "\n",
    "restore_original_weights(models, model_orig_weights)\n",
    "\n",
    "updated_params = {fixed_symbol: {permutee_symbol: None}}\n",
    "\n",
    "pylogger.info(f\"Permuting model {permutee_symbol} into {fixed_symbol}.\")\n",
    "\n",
    "# perms[a, b] maps b -> a\n",
    "updated_params[fixed_symbol][permutee_symbol] = apply_permutation_to_statedict(\n",
    "    permutation_spec, naive_permutations[fixed_symbol][permutee_symbol], models[permutee_symbol].model.state_dict()\n",
    ")\n",
    "restore_original_weights(models, model_orig_weights)\n",
    "\n",
    "lambdas = [0.0, 0.5, 1]  # np.linspace(0, 1, num=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_results = evaluate_pair_of_models(\n",
    "    models,\n",
    "    fixed_symbol,\n",
    "    permutee_symbol,\n",
    "    updated_params,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    lambdas,\n",
    "    core_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git Re-Basin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_original_weights(models, model_orig_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dicts for permutations and permuted params, D[a][b] refers to the permutation/params to map b -> a\n",
    "gitrebasin_permutations = {symb: {other_symb: None for other_symb in symbols.difference(symb)} for symb in symbols}\n",
    "\n",
    "matcher = GitRebasinMatcher(name=\"git_rebasin\", permutation_spec=permutation_spec)\n",
    "gitrebasin_permutations[fixed_symbol][permutee_symbol], perm_history = matcher(\n",
    "    fixed=fixed_model.model.cpu(), permutee=permutee_model.model.cpu()\n",
    ")\n",
    "\n",
    "gitrebasin_permutations[permutee_symbol][fixed_symbol] = get_inverse_permutations(\n",
    "    gitrebasin_permutations[fixed_symbol][permutee_symbol]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for perm in gitrebasin_permutations[fixed_symbol][permutee_symbol].values():\n",
    "\n",
    "#     plt.imshow(perm_indices_to_perm_matrix(perm), cmap=\"coolwarm\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.evaluate_matched_models import evaluate_pair_of_models\n",
    "\n",
    "restore_original_weights(models, model_orig_weights)\n",
    "\n",
    "updated_params = {fixed_symbol: {permutee_symbol: None}}\n",
    "\n",
    "pylogger.info(f\"Permuting model {permutee_symbol} into {fixed_symbol}.\")\n",
    "\n",
    "# perms[a, b] maps b -> a\n",
    "updated_params[fixed_symbol][permutee_symbol] = apply_permutation_to_statedict(\n",
    "    permutation_spec, gitrebasin_permutations[fixed_symbol][permutee_symbol], models[permutee_symbol].model.state_dict()\n",
    ")\n",
    "restore_original_weights(models, model_orig_weights)\n",
    "\n",
    "lambdas = [0.0, 0.5, 1.0]\n",
    "\n",
    "gitrebasin_results = evaluate_pair_of_models(\n",
    "    models,\n",
    "    fixed_symbol,\n",
    "    permutee_symbol,\n",
    "    updated_params,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    lambdas,\n",
    "    core_cfg,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_original_weights(models, model_orig_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"git_rebasin\": gitrebasin_results, \"naive\": naive_results, \"func\": func_results}\n",
    "# plot train and test acc\n",
    "\n",
    "for i, (method, method_results) in enumerate(results.items()):\n",
    "\n",
    "    test_acc = method_results[\"test_acc\"]\n",
    "    train_acc = method_results[\"train_acc\"]\n",
    "\n",
    "    plt.plot(lambdas, train_acc, label=f\"{method}\", linestyle=\"solid\", color=palette.get_colors(3)[i])\n",
    "    plt.plot(lambdas, test_acc, label=f\"{method}\", linestyle=\"dashed\", color=palette.get_colors(3)[i])\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
