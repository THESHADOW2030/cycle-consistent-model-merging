{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "import math\n",
    "import itertools\n",
    "from ccmm.utils.utils import l2_norm_models\n",
    "import hydra\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import omegaconf\n",
    "import seaborn as sns\n",
    "import torch  # noqa\n",
    "import wandb\n",
    "from hydra.utils import instantiate\n",
    "from matplotlib import tri\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning import LightningModule\n",
    "from scipy.stats import qmc\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from ccmm.matching.utils import perm_indices_to_perm_matrix\n",
    "from ccmm.utils.utils import normalize_unit_norm, project_onto\n",
    "from functools import partial\n",
    "\n",
    "from nn_core.callbacks import NNTemplateCore\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "from nn_core.common.utils import seed_index_everything\n",
    "from nn_core.model_logging import NNLogger\n",
    "from ccmm.utils.utils import fuse_batch_norm_into_conv\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import ccmm  # noqa\n",
    "from ccmm.matching.utils import (\n",
    "    apply_permutation_to_statedict,\n",
    "    get_all_symbols_combinations,\n",
    "    plot_permutation_history_animation,\n",
    "    restore_original_weights,\n",
    ")\n",
    "from ccmm.utils.utils import (\n",
    "    linear_interpolate,\n",
    "    load_model_from_info,\n",
    "    map_model_seed_to_symbol,\n",
    "    save_factored_permutations,\n",
    ")\n",
    "from ccmm.pl_modules.pl_module import MyLightningModule\n",
    "\n",
    "from ccmm.matching.utils import load_permutations\n",
    "\n",
    "from ccmm.utils.utils import vector_to_state_dict, get_interpolated_loss_acc_curves\n",
    "import pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as anp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymanopt\n",
    "import pymanopt.manifolds\n",
    "import pymanopt.optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "from scipy.linalg import eig\n",
    "from numpy.linalg import svd\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import scipy\n",
    "import json\n",
    "\n",
    "\n",
    "def build_laplacian(knn_graph, normalized=True):\n",
    "\n",
    "    A = (knn_graph + knn_graph.T).astype(float)\n",
    "    A = A.toarray()\n",
    "\n",
    "    D = np.diag(np.sum(A, axis=1))\n",
    "    L = D - A\n",
    "\n",
    "    if normalized:\n",
    "        D_inv_sqrt = np.diag(1 / (np.sqrt(np.diag(D)) + 1e-6))\n",
    "        L = D_inv_sqrt @ L @ D_inv_sqrt\n",
    "\n",
    "    evals, evecs = eig(L)\n",
    "    evals = evals.real\n",
    "\n",
    "    idx = evals.argsort()\n",
    "    evals = evals[idx]\n",
    "    evecs = evecs[:, idx]\n",
    "\n",
    "    return A, L, evals, evecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams[\"font.family\"] = \"serif\"\n",
    "sns.set_context(\"talk\")\n",
    "cmap_name = \"coolwarm_r\"\n",
    "\n",
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").setLevel(logging.WARNING)\n",
    "pylogger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from typing import Dict, List\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=str(\"../conf\"), job_name=\"matching_n_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"func_maps\", overrides=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_cfg = cfg  # NOQA\n",
    "cfg = cfg.matching\n",
    "\n",
    "seed_index_everything(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = -1\n",
    "num_train_samples = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = instantiate(core_cfg.dataset.test.transform)\n",
    "\n",
    "train_dataset = instantiate(core_cfg.dataset.train, transform=transform)\n",
    "test_dataset = instantiate(core_cfg.dataset.test, transform=transform)\n",
    "\n",
    "num_train_samples = len(train_dataset) if num_train_samples < 0 else num_train_samples\n",
    "\n",
    "train_subset = Subset(train_dataset, list(range(num_train_samples)))\n",
    "train_loader = DataLoader(train_subset, batch_size=1000, num_workers=cfg.num_workers)\n",
    "\n",
    "num_test_samples = len(test_dataset) if num_test_samples < 0 else num_test_samples\n",
    "test_subset = Subset(test_dataset, list(range(num_test_samples)))\n",
    "\n",
    "test_loader = DataLoader(test_subset, batch_size=1000, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = instantiate(cfg.trainer, enable_progress_bar=False, enable_model_summary=False, max_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input=28 * 28, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.input = input\n",
    "        self.layer0 = nn.Linear(input, 512)\n",
    "        self.layer1 = nn.Linear(512, 512)\n",
    "        self.layer2 = nn.Linear(512, 512)\n",
    "        self.layer3 = nn.Linear(512, 256)\n",
    "        self.layer4 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input)\n",
    "\n",
    "        h0 = nn.functional.relu(self.layer0(x))\n",
    "\n",
    "        h1 = nn.functional.relu(self.layer1(h0))\n",
    "\n",
    "        h2 = nn.functional.relu(self.layer2(h1))\n",
    "\n",
    "        h3 = nn.functional.relu(self.layer3(h2))\n",
    "\n",
    "        h4 = self.layer4(h3)\n",
    "\n",
    "        embeddings = [h0, h1, h2, h3, h4]\n",
    "\n",
    "        return nn.functional.log_softmax(h4, dim=-1), embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.matching.permutation_spec import MLPPermutationSpecBuilder\n",
    "\n",
    "permutation_spec_builder = MLPPermutationSpecBuilder(4)\n",
    "permutation_spec = permutation_spec_builder.create_permutation_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.seed_index = 0\n",
    "seed_index_everything(cfg)\n",
    "model_a = MyLightningModule(MLP(), num_classes=10)\n",
    "\n",
    "trainer = instantiate(cfg.trainer, enable_progress_bar=True, enable_model_summary=False, max_epochs=20)\n",
    "trainer.fit(model_a, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.seed_index = 1\n",
    "seed_index_everything(cfg)\n",
    "\n",
    "model_b = MyLightningModule(MLP(), num_classes=10)\n",
    "trainer = instantiate(cfg.trainer, enable_progress_bar=True, enable_model_summary=False, max_epochs=20)\n",
    "trainer.fit(model_b, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.matching.weight_matching import weight_matching\n",
    "\n",
    "permutations = weight_matching(permutation_spec, model_a.model.state_dict(), model_b.model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_activations = 10000\n",
    "train_loader = DataLoader(train_subset, batch_size=num_activations, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "\n",
    "    x, y = batch\n",
    "    features_a = model_a.model(x)[-1]\n",
    "    features_b = model_b.model(x)[-1]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on a single layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 1\n",
    "\n",
    "perm_gt = permutations[f\"P_{layer_idx}\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (descriptor_dim, num_neurons), where descriptor_dim is the number of samples for which we are considering the neuron activation\n",
    "layer_a = features_a[layer_idx]\n",
    "layer_b = features_b[layer_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize to have unit norm\n",
    "\n",
    "layer_a = layer_a / (torch.norm(layer_a, dim=0) + 1e-6)\n",
    "layer_b = layer_b / (torch.norm(layer_b, dim=0) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(layer_a.shape, layer_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.utils.utils import to_np\n",
    "\n",
    "layer_a = to_np(layer_a)\n",
    "layer_b = to_np(layer_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git Re-Basin style matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_a_weights = model_a.model.state_dict()[f\"layer{layer_idx}.weight\"]\n",
    "layer_b_weights = model_b.model.state_dict()[f\"layer{layer_idx}.weight\"]\n",
    "\n",
    "sim_matrix_git_rebasin = layer_a_weights @ layer_b_weights.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ci = linear_sum_assignment(sim_matrix_git_rebasin.detach().numpy(), maximize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_matrix = perm_indices_to_perm_matrix(torch.tensor(ci)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_perm = perm_matrix @ layer_b_weights.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(layer_a @ b_perm.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (num_samples, num_neurons)\n",
    "layer_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def svd_threshold(matrix, variance_threshold=0.99):\n",
    "    # Compute SVD\n",
    "    U, S, Vt = np.linalg.svd(matrix, full_matrices=False)\n",
    "\n",
    "    # Calculate the cumulative variance explained by the singular values\n",
    "    total_variance = np.sum(S**2)\n",
    "    explained_variance = np.cumsum(S**2) / total_variance\n",
    "\n",
    "    # Determine the number of singular values needed to explain the desired threshold of variance\n",
    "    num_components = np.argmax(explained_variance >= variance_threshold) + 1\n",
    "\n",
    "    # Select the subset of singular values and vectors explaining the desired variance\n",
    "    U_reduced = U[:, :num_components]\n",
    "    S_reduced = S[:num_components]\n",
    "    Vt_reduced = Vt[:num_components, :]\n",
    "\n",
    "    return U_reduced, S_reduced, Vt_reduced, explained_variance\n",
    "\n",
    "\n",
    "def svd_num_components(matrix, num_components=10):\n",
    "    # matrix is ~ (num_samples, num_neurons)\n",
    "    num_samples, num_neurons = matrix.shape\n",
    "    K = num_components\n",
    "\n",
    "    U, S, Vt = np.linalg.svd(matrix, full_matrices=False)\n",
    "\n",
    "    assert U.shape == (num_samples, num_neurons)\n",
    "    assert S.shape == (num_neurons,)\n",
    "    assert Vt.shape == (num_neurons, num_neurons)\n",
    "\n",
    "    U_reduced = U[:, :K]\n",
    "    S_reduced = S[:K]\n",
    "    Vt_reduced = Vt[:K, :]\n",
    "\n",
    "    return U_reduced, S_reduced, Vt_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (num_neurons, num_comps), (num_comps), (num_comps, num_samples)\n",
    "U_a, S_a, Vt_a = svd_num_components(layer_a, num_components=128)\n",
    "U_b, S_b, Vt_b = svd_num_components(layer_b, num_components=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(U_a.shape, S_a.shape, Vt_a.shape)\n",
    "print(U_b.shape, S_b.shape, Vt_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenneurons_a = 1 / ((np.diag(S_a) ** 0.5) + 1 - 6) @ Vt_a\n",
    "eigenneurons_b = 1 / ((np.diag(S_b) ** 0.5) + 1 - 6) @ Vt_b\n",
    "\n",
    "eigenneurons_a = eigenneurons_a.T\n",
    "eigenneurons_b = eigenneurons_b.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# express each layer as a linear combination of the singular vectors\n",
    "layer_a_reconstructed = U_a @ np.diag(S_a) @ Vt_a\n",
    "layer_b_reconstructed = U_b @ np.diag(S_b) @ Vt_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_a_reconstructed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the reconstruction is close to the original layer by computing the norm\n",
    "np.linalg.norm(layer_a_reconstructed - layer_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the reconstruction is close to the original layer by computing the norm\n",
    "np.linalg.norm(layer_b_reconstructed - layer_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the norm of the two models for comparison\n",
    "np.linalg.norm(layer_a - layer_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_a = layer_a_weights.detach().numpy()\n",
    "W_b = layer_b_weights.detach().numpy()\n",
    "\n",
    "W_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (num_samples, num_neurons)\n",
    "layer_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a 7x7 grid of subplots, one with each func map\n",
    "\n",
    "\n",
    "def plot_func_maps(func_maps, fig_name, vmin, vmax):\n",
    "    fig, axs = plt.subplots(7, 7, figsize=(20, 20))\n",
    "\n",
    "    for i in range(7):\n",
    "        for j in range(7):\n",
    "\n",
    "            ax = axs[i, j]\n",
    "            ax.imshow(func_maps[i * 7 + j], cmap=cmap_name, vmin=vmin, vmax=vmax)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "    plt.savefig(f\"figures/{fig_name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_func_map(X, Y, P, radius=None, num_neighbors=None, mode=\"distance\", normalize_lap=True):\n",
    "\n",
    "    assert radius is not None or num_neighbors is not None\n",
    "\n",
    "    if radius is not None:\n",
    "        Xneigh = NearestNeighbors(radius=radius)\n",
    "        Yneigh = NearestNeighbors(radius=radius)\n",
    "\n",
    "    elif num_neighbors is not None:\n",
    "        Xneigh = NearestNeighbors(n_neighbors=num_neighbors)\n",
    "        Yneigh = NearestNeighbors(n_neighbors=num_neighbors)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Either radius or num_neighbors must be provided\")\n",
    "\n",
    "    Xneigh.fit(X)\n",
    "    # (num_neurons, num_neurons)\n",
    "    X_knn_graph = Xneigh.kneighbors_graph(X, mode=mode)\n",
    "\n",
    "    Yneigh.fit(Y)\n",
    "    Y_knn_graph = Yneigh.kneighbors_graph(Y, mode=mode)\n",
    "\n",
    "    XA, XL, Xevals, Xevecs = build_laplacian(X_knn_graph, normalize_lap)\n",
    "    YA, YL, Yevals, Yevecs = build_laplacian(Y_knn_graph, normalize_lap)\n",
    "\n",
    "    Xevecs = Xevecs.real\n",
    "    Yevecs = Yevecs.real\n",
    "\n",
    "    num_eigenvectors = 50\n",
    "    C = Xevecs[:, :num_eigenvectors].T @ P @ Yevecs[:, :num_eigenvectors]\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptors = \"eigenneurons\"  # weights, features, features_denoised\n",
    "\n",
    "if descriptors == \"weights\":\n",
    "    X, Y = W_a, W_b\n",
    "elif descriptors == \"features\":\n",
    "    X, Y = layer_a.T, layer_b.T\n",
    "elif descriptors == \"features_denoised\":\n",
    "    X, Y = layer_a_reconstructed.T, layer_b_reconstructed.T\n",
    "elif descriptors == \"eigenneurons\":\n",
    "    X, Y = eigenneurons_a, eigenneurons_b\n",
    "else:\n",
    "    raise ValueError(\"Invalid value for use_weights_or_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = perm_indices_to_perm_matrix(perm_gt).numpy()\n",
    "normalize_lap = True\n",
    "mode = \"connectivity\"  # connectivity or distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_maps_neighbors = [\n",
    "    compute_func_map(X, Y, P, num_neighbors=k, mode=mode, normalize_lap=normalize_lap) for k in range(1, 100, 2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_name = f\"func_maps_{descriptors}_{mode}_normalizeLap_{normalize_lap}\"\n",
    "plot_func_maps(func_maps_neighbors, plot_name, vmin=-0.3, vmax=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute diameter of X\n",
    "\n",
    "# compute cidst with numpy\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "diameter = np.max(cdist(X.T, X.T, metric=\"euclidean\"))\n",
    "print(diameter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_maps_radius = [compute_func_map(X, Y, P, radius=r) for r in np.linspace(0.01, 100, 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_func_maps(func_maps_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "\n",
    "# (num_neurons, num_samples)\n",
    "# X, Y = layer_a_reconstructed.T, layer_b_reconstructed.T\n",
    "X, Y = W_a, W_b\n",
    "\n",
    "Xneigh = NearestNeighbors(n_neighbors=k)\n",
    "Xneigh.fit(X)\n",
    "\n",
    "# (num_neurons, num_neurons)\n",
    "X_knn_graph = Xneigh.kneighbors_graph(X, mode=\"connectivity\")\n",
    "\n",
    "Yneigh = NearestNeighbors(n_neighbors=k)\n",
    "Yneigh.fit(Y)\n",
    "Y_knn_graph = Yneigh.kneighbors_graph(Y, mode=\"connectivity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(X.T)\n",
    "\n",
    "Xx = pca.components_[0, :]\n",
    "Xy = pca.components_[1, :]\n",
    "Xz = pca.components_[2, :]\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(Y.T)\n",
    "\n",
    "Yx = pca.components_[0, :]\n",
    "Yy = pca.components_[1, :]\n",
    "Yz = pca.components_[2, :]\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0] = fig.add_subplot(121, projection=\"3d\")\n",
    "ax[0].scatter(Xx, Xy, Xz, c=\"tab:blue\")\n",
    "\n",
    "ax[1] = fig.add_subplot(122, projection=\"3d\")\n",
    "ax[1].scatter(Yx, Yy, Yz, c=\"tab:red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax[0] = fig.add_subplot(121, projection=\"3d\")\n",
    "\n",
    "num_neurons = W_a.shape[0]\n",
    "for i in range(num_neurons):\n",
    "    for j in range(num_neurons):\n",
    "        if X_knn_graph[i, j] > 0:\n",
    "            ax[0].plot([Xx[i], Xx[j]], [Xy[i], Xy[j]], [Xz[i], Xz[j]], \"b-\", alpha=0.5)\n",
    "\n",
    "ax[0].scatter(Xx, Xy, Xz, c=\"tab:blue\")\n",
    "\n",
    "ax[1] = fig.add_subplot(122, projection=\"3d\")\n",
    "for i in range(num_neurons):\n",
    "    for j in range(num_neurons):\n",
    "        if Y_knn_graph[i, j] > 0:\n",
    "            ax[1].plot([Yx[i], Yx[j]], [Yy[i], Yy[j]], [Yz[i], Yz[j]], \"b-\", alpha=0.5)\n",
    "\n",
    "ax[1].scatter(Yx, Yy, Yz, c=\"tab:red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XA, XL, Xevals, Xevecs = build_laplacian(X_knn_graph, True)\n",
    "YA, YL, Yevals, Yevecs = build_laplacian(Y_knn_graph, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P = np.eye(num_neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K = 50\n",
    "# C = Xevecs[:, :K].T @ P.T @ Yevecs[:, :K]\n",
    "# C_Pt = Xevecs[:, :K].T @ P @ Yevecs[:, :K]\n",
    "\n",
    "# fig, ax = plt.subplots(1, 3, figsize=(16, 8))\n",
    "# ax[0].imshow(P)\n",
    "# ax[1].imshow(C[1:,1:], cmap='bwr')\n",
    "# ax[2].imshow(C_Pt[1:, 1:], cmap='bwr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = num_neurons\n",
    "# k = 24\n",
    "\n",
    "# def project_sinkhorn(P, max_iters=100):\n",
    "#     Q = P\n",
    "#     for _ in range(max_iters):\n",
    "#         r = np.sum(Q, axis=0)\n",
    "#         Q = Q @ np.diag(1 / r)\n",
    "#         c = np.sum(Q, axis=1)\n",
    "#         Q = np.diag(1 / c) @ Q\n",
    "\n",
    "#     return Q\n",
    "\n",
    "# manifold = pymanopt.manifolds.Positive(n, n)\n",
    "\n",
    "# @pymanopt.function.autograd(manifold)\n",
    "# def cost(point):\n",
    "#     C = Xevecs[:,1:k].T @ point @ Yevecs[:,1:k]\n",
    "#     return anp.sum((C - anp.diag(anp.diag(C))) ** 2)\n",
    "\n",
    "# optimizer = pymanopt.optimizers.SteepestDescent()\n",
    "# problem = pymanopt.Problem(manifold, cost)\n",
    "\n",
    "# P = None\n",
    "# for outer_iter in range(10):\n",
    "#     result = optimizer.run(problem, initial_point=P)\n",
    "#     P = project_sinkhorn(result.point)\n",
    "\n",
    "# C = Xevecs[:,1:k].T @ P @ Yevecs[:,1:k]\n",
    "# fig, ax = plt.subplots(1, 2, figsize=(4,2))\n",
    "# ax[0].imshow(P)\n",
    "# ax[1].imshow(C, cmap='bwr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve a LAP in the reduced space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "# _, ci = linear_sum_assignment(U_a.T @ U_b + Vt_a.T @ Vt_b.T, maximize=True)\n",
    "_, ci = linear_sum_assignment(layer_a_reconstructed.T @ layer_b_reconstructed, maximize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_matrix = perm_indices_to_perm_matrix(torch.tensor(ci)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U_sigma_b_perm =  U_b @ perm_matrix\n",
    "# Vt_b = perm_matrix @ Vt_b\n",
    "# S_b = perm_matrix @ S_b\n",
    "\n",
    "# layer_b_reconstructed_perm = U_sigma_b_perm @ np.diag(S_b) @ Vt_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_b_reconstructed_perm = perm_matrix @ layer_b_reconstructed.T\n",
    "\n",
    "layer_b_reconstructed_perm = layer_b_reconstructed_perm.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_b_recon_perm_norm = layer_b_reconstructed_perm / (np.linalg.norm(layer_b_reconstructed_perm, axis=0) + 1e-6)\n",
    "layer_a_norm = layer_a / (np.linalg.norm(layer_a, axis=0) + 1e-6)\n",
    "layer_b_norm = layer_b / (np.linalg.norm(layer_b, axis=0) + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(layer_b_recon_perm_norm.T @ layer_a_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.trace(layer_b_norm.T @ layer_a_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LAP in the original space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix_orig_space = layer_a @ layer_b.T\n",
    "\n",
    "_, ci = linear_sum_assignment(-sim_matrix_orig_space, maximize=True)\n",
    "perm_matrix = perm_indices_to_perm_matrix(torch.tensor(ci)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_b_perm = perm_matrix @ layer_b\n",
    "\n",
    "layer_b_perm_norm = layer_b_perm / (np.linalg.norm(layer_b_perm, axis=0) + 1e-6)\n",
    "np.trace(layer_a_norm @ layer_b_perm_norm.T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
