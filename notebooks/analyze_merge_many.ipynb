{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "from functools import partial\n",
    "from pprint import pprint\n",
    "\n",
    "import hydra\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import omegaconf\n",
    "import pytorch_lightning\n",
    "import seaborn as sns\n",
    "import torch  # noqa\n",
    "import wandb\n",
    "from hydra.utils import instantiate\n",
    "from matplotlib import tri\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning import LightningModule\n",
    "from scipy.stats import qmc\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nn_core.callbacks import NNTemplateCore\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "from nn_core.common.utils import seed_index_everything\n",
    "from nn_core.model_logging import NNLogger\n",
    "\n",
    "import ccmm  # noqa\n",
    "from ccmm.matching.frank_wolfe_sync_matching import frank_wolfe_synchronized_matching\n",
    "from ccmm.matching.utils import (\n",
    "    apply_permutation_to_statedict,\n",
    "    get_all_symbols_combinations,\n",
    "    load_permutations,\n",
    "    perm_indices_to_perm_matrix,\n",
    "    perm_matrix_to_perm_indices,\n",
    "    plot_permutation_history_animation,\n",
    "    restore_original_weights,\n",
    ")\n",
    "from ccmm.utils.utils import (\n",
    "    fuse_batch_norm_into_conv,\n",
    "    load_model_from_info,\n",
    "    map_model_seed_to_symbol,\n",
    "    normalize_unit_norm,\n",
    "    project_onto,\n",
    "    save_factored_permutations,\n",
    "    vector_to_state_dict,\n",
    ")\n",
    "from ccmm.matching.weight_matching import solve_linear_assignment_problem\n",
    "from ccmm.matching.utils import unfactor_permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams[\"font.family\"] = \"serif\"\n",
    "sns.set_context(\"talk\")\n",
    "matplotlib.rcParams[\"text.usetex\"] = True\n",
    "cmap_name = \"coolwarm_r\"\n",
    "\n",
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").setLevel(logging.WARNING)\n",
    "pylogger = logging.getLogger(__name__)\n",
    "\n",
    "from ccmm.utils.plot import Palette\n",
    "\n",
    "palette = Palette(f\"{PROJECT_ROOT}/misc/palette2.json\")\n",
    "palette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def average_models(model_params, reduction_fn, coeffs=None):\n",
    "    if not isinstance(model_params, List):\n",
    "        model_params = list(model_params.values())\n",
    "\n",
    "    print(coeffs)\n",
    "    if coeffs is None:\n",
    "        coeffs = [1] * len(model_params)\n",
    "\n",
    "    return {\n",
    "        k: reduction_fn(torch.stack([p[k] * coeffs[i] for i, p in enumerate(model_params)]))\n",
    "        for k in model_params[0].keys()\n",
    "    }\n",
    "\n",
    "\n",
    "def trimmed_mean(tensors, trim_ratio=0.1):\n",
    "    num_values = tensors.size(0)\n",
    "    num_to_trim = int(trim_ratio * num_values)\n",
    "    sorted_tensors = tensors.sort(dim=0).values\n",
    "    trimmed_tensors = sorted_tensors[num_to_trim : num_values - num_to_trim]\n",
    "    return trimmed_tensors.mean(dim=0)\n",
    "\n",
    "\n",
    "def winsorize(tensor, limits=[0.2, 0.8]):\n",
    "    lower, upper = torch.quantile(tensor, torch.tensor(limits).float(), dim=0)\n",
    "    clipped = torch.clamp(tensor, min=lower, max=upper)\n",
    "    return clipped.mean(dim=0)\n",
    "\n",
    "\n",
    "def robust_mean(tensor, threshold=3.5):\n",
    "    median_val = tensor.median(dim=0).values\n",
    "    mad_val = (tensor - median_val).abs().median(dim=0).values\n",
    "    mad_val[mad_val == 0] = 1  # Prevent division by zero\n",
    "    z_score = 0.6745 * (tensor - median_val) / mad_val\n",
    "    mask = (z_score.abs() < threshold).float()  # Create a mask to zero-out outliers\n",
    "    filtered_tensor = tensor * mask  # Apply mask\n",
    "    robust_mean_val = filtered_tensor.sum(dim=0) / mask.sum(dim=0)  # Compute mean only over non-outlier values\n",
    "    return robust_mean_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from typing import Dict, List\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=str(\"../conf\"), job_name=\"merge_n_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"merge_n_models\", overrides=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_cfg = cfg  # NOQA\n",
    "cfg = cfg.matching\n",
    "\n",
    "seed_index_everything(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_samples = 5000\n",
    "num_train_samples = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = instantiate(core_cfg.dataset.test.transform)\n",
    "\n",
    "train_dataset = instantiate(core_cfg.dataset.train, transform=transform)\n",
    "test_dataset = instantiate(core_cfg.dataset.test, transform=transform)\n",
    "\n",
    "train_subset = Subset(train_dataset, list(range(num_train_samples)))\n",
    "train_loader = DataLoader(train_subset, batch_size=5000, num_workers=cfg.num_workers)\n",
    "\n",
    "test_subset = Subset(test_dataset, list(range(num_test_samples)))\n",
    "\n",
    "test_loader = DataLoader(test_subset, batch_size=1000, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = instantiate(cfg.trainer, enable_progress_bar=False, enable_model_summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.utils.utils import load_model_from_artifact\n",
    "\n",
    "run = wandb.init(project=core_cfg.core.project_name, entity=core_cfg.core.entity, job_type=\"matching\")\n",
    "\n",
    "# {a: 1, b: 2, c: 3, ..}\n",
    "symbols_to_seed: Dict[int, str] = {map_model_seed_to_symbol(seed): seed for seed in cfg.model_seeds}\n",
    "\n",
    "artifact_path = (\n",
    "    lambda seed: f\"{core_cfg.core.entity}/{core_cfg.core.project_name}/{core_cfg.dataset.name}_{core_cfg.model.model_identifier}_{seed}:v0\"\n",
    ")\n",
    "\n",
    "# {a: model_a, b: model_b, c: model_c, ..}\n",
    "models: Dict[str, LightningModule] = {\n",
    "    map_model_seed_to_symbol(seed): load_model_from_artifact(run, artifact_path(seed)) for seed in cfg.model_seeds\n",
    "}\n",
    "\n",
    "num_models = len(models)\n",
    "\n",
    "pylogger.info(f\"Using {num_models} models with architecture {core_cfg.model.model_identifier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always permute the model having larger character order, i.e. c -> b, b -> a and so on ...\n",
    "symbols = set(symbols_to_seed.keys())\n",
    "symbols = symbols.difference({\"o\"})  # \"o\" is the model trained over all the dataset\n",
    "\n",
    "sorted_symbols = sorted(symbols, reverse=False)\n",
    "\n",
    "# (a, b), (a, c), (b, c), ...\n",
    "all_combinations = get_all_symbols_combinations(symbols)\n",
    "# combinations of the form (a, b), (a, c), (b, c), .. and not (b, a), (c, a) etc\n",
    "canonical_combinations = [(source, target) for (source, target) in all_combinations if source < target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_fns = {\"mean\": partial(torch.mean, dim=0), \"trimmed\": trimmed_mean, \"winsor\": winsorize, \"filter\": robust_mean}\n",
    "\n",
    "chosen_red_fn = red_fns[\"mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylogger.info(f\"Matching the following model pairs: {canonical_combinations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load permutation specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_spec_builder = instantiate(core_cfg.model.permutation_spec_builder)\n",
    "permutation_spec = permutation_spec_builder.create_permutation()\n",
    "\n",
    "ref_model = list(models.values())[0]\n",
    "assert set(permutation_spec.layer_and_axes_to_perm.keys()) == set(ref_model.model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_merger = instantiate(cfg.merger, permutation_spec=permutation_spec)\n",
    "\n",
    "pylogger.info(f\"Merger: {model_merger.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 200\n",
    "initialization_method = \"identity\"\n",
    "keep_soft_perms = True\n",
    "\n",
    "symbols = list(models.keys())\n",
    "\n",
    "merged_model = copy.deepcopy(models[symbols[0]])\n",
    "\n",
    "perm_indices, opt_infos = frank_wolfe_synchronized_matching(\n",
    "    models=models,\n",
    "    perm_spec=permutation_spec,\n",
    "    symbols=symbols,\n",
    "    combinations=canonical_combinations,\n",
    "    max_iter=max_iter,\n",
    "    initialization_method=initialization_method,\n",
    "    keep_soft_perms=keep_soft_perms,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot intermediate permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "plot_permutation_history_animation = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if plot_permutation_history_animation:\n",
    "    perm_symbols = list(perm_indices[\"a\"].keys())\n",
    "    # perm_symbols = ['P_bg0', 'P_blockgroup3.block2_inner']\n",
    "    k = 0\n",
    "    K = 5\n",
    "    for perm_symb in perm_symbols:\n",
    "        perms = {symb: [perm[symb][perm_symb] for perm in opt_infos[\"perm_history\"][k:K]] for symb in symbols}\n",
    "\n",
    "        fig, ax = plt.subplots(5, K - k, figsize=(20, 20))\n",
    "\n",
    "        for i in range(K - k):\n",
    "            for j, symb in enumerate(symbols):\n",
    "                ax[j, i].imshow(perms[symb][i].cpu(), cmap=\"gray\")\n",
    "                ax[j, i].axis(\"off\")\n",
    "                ax[j, i].set_title(symb)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_symbols = list(perm_indices[\"a\"].keys())\n",
    "print(perm_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot convergence values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(opt_infos[\"obj_values\"], color=palette[\"light red\"])\n",
    "\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Objective\")\n",
    "plt.title(\"Objective curve\")\n",
    "\n",
    "plt.savefig(\"figures/convergence_obj.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([step_size for ind, step_size in enumerate(opt_infos[\"step_sizes\"])], color=palette[\"light red\"])\n",
    "\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Step size\")\n",
    "plt.title(\"Step size\")\n",
    "\n",
    "plt.savefig(\"figures/convergence_step_sizes.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [step_size for ind, step_size in enumerate(opt_infos[\"step_sizes\"]) if ind % 2 == 1], color=palette[\"light red\"]\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Step size\")\n",
    "plt.title(\"Step size for odd iterations\")\n",
    "\n",
    "plt.savefig(\"figures/convergence_step_sizes_odd.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(\n",
    "    [step_size for ind, step_size in enumerate(opt_infos[\"step_sizes\"]) if ind % 2 == 0], color=palette[\"light red\"]\n",
    ")\n",
    "plt.ylabel(\"Step size\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.title(\"Step size for even iterations\")\n",
    "plt.savefig(\"figures/convergence_step_sizes_even.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot soft vs hard perms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_perms = {\n",
    "    symb: {p: solve_linear_assignment_problem(perm) for p, perm in perm_indices[symb].items()} for symb in symbols\n",
    "}\n",
    "\n",
    "soft_perms = copy.deepcopy(perm_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perms = False\n",
    "if plot_perms:\n",
    "    perm_symbols = list(perm_indices[\"a\"].keys())\n",
    "\n",
    "    for perm_symbol in perm_symbols:\n",
    "        fig, axs = plt.subplots(2, len(symbols), figsize=(40, 20))\n",
    "        fig.suptitle(f\"Permutation: {perm_symbol}\", fontsize=16)\n",
    "        for i, symbol in enumerate(symbols):\n",
    "\n",
    "            ax0 = axs[0][i]\n",
    "            ax0.set_title(symbol)\n",
    "            ax0.imshow(soft_perms[symbol][perm_symbol].cpu(), cmap=cmap_name)\n",
    "            ax0.colorbar = plt.colorbar(\n",
    "                matplotlib.cm.ScalarMappable(norm=colors.Normalize(vmin=0, vmax=1), cmap=cmap_name), ax=ax0\n",
    "            )\n",
    "\n",
    "            ax1 = axs[1][i]\n",
    "            ax1.set_title(symbol)\n",
    "            ax1.imshow(perm_indices_to_perm_matrix(hard_perms[symbol][perm_symbol]), cmap=cmap_name)\n",
    "            ax1.colorbar = plt.colorbar(\n",
    "                matplotlib.cm.ScalarMappable(norm=colors.Normalize(vmin=0, vmax=1), cmap=cmap_name), ax=ax1\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count non-zero values in soft perms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_name = \"P_blockgroup2.block3_inner\"\n",
    "nonzero_idxs = (soft_perms[\"a\"][perm_name] > 0) & (soft_perms[\"a\"][perm_name] < 1)\n",
    "\n",
    "soft_perms[\"a\"][perm_name][nonzero_idxs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map to universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models_permuted_to_univ(perms, models, symbols, keep_soft_perms=False):\n",
    "    models_permuted_to_universe = {symbol: copy.deepcopy(model) for symbol, model in models.items()}\n",
    "\n",
    "    for symbol in symbols:\n",
    "        perms_to_apply = {}\n",
    "\n",
    "        for perm_name in perms[symbol].keys():\n",
    "            perm = perms[symbol][perm_name]\n",
    "\n",
    "            if keep_soft_perms:\n",
    "                perm = perm.T\n",
    "                perm_to_apply = perm\n",
    "            else:\n",
    "                perm = perm_indices_to_perm_matrix(perm).T\n",
    "                perm_to_apply = perm_matrix_to_perm_indices(perm)\n",
    "\n",
    "            perms_to_apply[perm_name] = perm_to_apply\n",
    "\n",
    "        updated_params = apply_permutation_to_statedict(\n",
    "            permutation_spec, perms_to_apply, models[symbol].model.state_dict()\n",
    "        )\n",
    "        models_permuted_to_universe[symbol].model.load_state_dict(updated_params)\n",
    "\n",
    "    return models_permuted_to_universe\n",
    "\n",
    "\n",
    "models_permuted_to_universe = get_models_permuted_to_univ(soft_perms, models, symbols, keep_soft_perms=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_permuted_pairwise = {\n",
    "    symbol: {other_symb: None for other_symb in set(symbols).difference(symbol)} for symbol in symbols\n",
    "}\n",
    "pairwise_permutations = unfactor_permutations(hard_perms)\n",
    "\n",
    "for fixed, permutee in all_combinations:\n",
    "    ref_model = copy.deepcopy(models[\"a\"])\n",
    "\n",
    "    permuted_params = apply_permutation_to_statedict(\n",
    "        permutation_spec, pairwise_permutations[fixed][permutee], models[permutee].model.state_dict()\n",
    "    )\n",
    "\n",
    "    ref_model.model.load_state_dict(permuted_params)\n",
    "    models_permuted_pairwise[fixed][permutee] = ref_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {symb: model.to(\"cpu\") for symb, model in models.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT: cosine similarities in original and universe space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_models = {symbol: torch.nn.utils.parameters_to_vector(model.parameters()) for symbol, model in models.items()}\n",
    "flat_models_permuted_to_universe = {\n",
    "    symbol: torch.nn.utils.parameters_to_vector(model.parameters())\n",
    "    for symbol, model in models_permuted_to_universe.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_similarities(models, filename, dist=\"cosine\"):\n",
    "\n",
    "    dist_matrix = np.zeros((len(models), len(models)))\n",
    "\n",
    "    for i, (symbol_i, model_i) in enumerate(models.items()):\n",
    "        for j, (symbol_j, model_j) in enumerate(models.items()):\n",
    "\n",
    "            if dist == \"cosine\":\n",
    "                dist_matrix[i, j] = (model_i @ model_j) / (torch.norm(model_i) * torch.norm(model_j))\n",
    "                title = \"Cosine Similarity\"\n",
    "                fmt = \".2g\"\n",
    "                vmin, vmax = 0, 1\n",
    "\n",
    "            elif dist == \"euclidean\":\n",
    "                dist_matrix[i, j] = torch.norm(model_i - model_j)\n",
    "\n",
    "                title = \"Euclidean Distance\"\n",
    "                fmt = \".4g\"\n",
    "\n",
    "                vmin, vmax = 0, 85\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown distance metric: {dist}\")\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "\n",
    "    cmap = sns.light_palette(\"seagreen\", as_cmap=True)\n",
    "\n",
    "    mask = np.triu(np.ones_like(dist_matrix), k=1)\n",
    "\n",
    "    sns.heatmap(dist_matrix, annot=True, cmap=cmap, cbar=False, mask=mask, vmin=vmin, vmax=vmax, fmt=fmt)\n",
    "    plt.ylabel(\"Model Symbol\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(f\"figures/{filename}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_similarities = False\n",
    "if plot_similarities:\n",
    "    dist = \"euclidean\"\n",
    "    plot_similarities(flat_models, filename=f\"similarities_orig_{dist}.pdf\", dist=dist)\n",
    "    plot_similarities(flat_models_permuted_to_universe, filename=f\"similarities_univ_{dist}.pdf\", dist=dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT: CKA of original and universe space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from latentis.measure.functional.cka import linear_cka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_repr_similarities(models, dataset, filename, dist=\"cka\"):\n",
    "\n",
    "    dist_matrix = np.zeros((len(models), len(models)))\n",
    "\n",
    "    for i, (symbol_i, model_i) in enumerate(models.items()):\n",
    "        for j, (symbol_j, model_j) in enumerate(models.items()):\n",
    "            # model_params = [model_i.model.state_dict(), model_j.model.state_dict()]\n",
    "            # interp_model = average_models(model_params, reduction_fn=red_fns[\"mean\"])\n",
    "\n",
    "            num_activations = 1000\n",
    "            train_loader = DataLoader(dataset, batch_size=num_activations, num_workers=0)\n",
    "            batch = next(iter(train_loader))\n",
    "            x, y = batch\n",
    "            features_i = model_i.model(x, return_embeddings=True)\n",
    "            features_j = model_j.model(x, return_embeddings=True)\n",
    "\n",
    "            if dist == \"cka\":\n",
    "                dist_matrix[i, j] = linear_cka(features_i, features_j)\n",
    "                title = \"Centered Kernel Alignment\"\n",
    "                fmt = \".2g\"\n",
    "                vmin, vmax = 0, 1\n",
    "            elif dist == \"euclidean\":\n",
    "                dist_matrix[i, j] = torch.norm(features_i - features_j)\n",
    "                title = \"Euclidean distance between representations\"\n",
    "                fmt = \".4g\"\n",
    "                vmin, vmax = 0, 300\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown distance metric: {dist}\")\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "\n",
    "    cmap = sns.light_palette(\"seagreen\", as_cmap=True)\n",
    "\n",
    "    mask = np.triu(np.ones_like(dist_matrix), k=1)\n",
    "\n",
    "    sns.heatmap(dist_matrix, annot=True, cmap=cmap, cbar=False, mask=mask, vmin=vmin, vmax=vmax, fmt=fmt)\n",
    "    plt.ylabel(\"Model Symbol\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(f\"figures/{filename}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_repr_similarities = False\n",
    "\n",
    "if plot_repr_similarities:\n",
    "\n",
    "    dist = \"euclidean\"\n",
    "    plot_repr_similarities(models, dataset=train_dataset, filename=f\"similarities_orig_repr_{dist}.pdf\", dist=dist)\n",
    "    plot_repr_similarities(\n",
    "        models_permuted_to_universe, dataset=train_dataset, filename=f\"similarities_univ_repr_{dist}.pdf\", dist=dist\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PLOT: performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_performance_matrix(models, loader, filename):\n",
    "\n",
    "    dist_matrix = np.zeros((len(models), len(models)))\n",
    "\n",
    "    for i, (symbol_i, model_i) in enumerate(models.items()):\n",
    "        for j, (symbol_j, model_j) in enumerate(models.items()):\n",
    "            ref_model = copy.deepcopy(list(models.values())[0])\n",
    "\n",
    "            model_params = [model_i.model.state_dict(), model_j.model.state_dict()]\n",
    "            interp_model = average_models(model_params, reduction_fn=red_fns[\"mean\"])\n",
    "            ref_model.model.load_state_dict(interp_model)\n",
    "\n",
    "            res = trainer.test(ref_model, loader)[0]\n",
    "            dist_matrix[i, j] = res[\"acc/test\"]\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "\n",
    "    cmap = sns.light_palette(\"seagreen\", as_cmap=True)\n",
    "\n",
    "    mask = np.triu(np.ones_like(dist_matrix), k=1)\n",
    "\n",
    "    vmin, vmax = 0, 1\n",
    "    fmt = \".2g\"\n",
    "    title = \"Interpolated models accuracy\"\n",
    "\n",
    "    sns.heatmap(dist_matrix, annot=True, cmap=cmap, cbar=False, mask=mask, vmin=vmin, vmax=vmax, fmt=fmt)\n",
    "    plt.ylabel(\"Model Symbol\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(f\"figures/{filename}\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_performance = False\n",
    "\n",
    "if plot_performance:\n",
    "    plot_performance_matrix(models, loader=test_loader, filename=\"performance_orig.pdf\")\n",
    "    plot_performance_matrix(models_permuted_to_universe, loader=test_loader, filename=\"performance_univ.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = [model.model.state_dict() for model in models_permuted_to_universe.values()]\n",
    "\n",
    "merged_params = average_models(model_params, reduction_fn=chosen_red_fn)\n",
    "merged_model.model.load_state_dict(merged_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = test_loader\n",
    "trainer.test(merged_model, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.matching.repair import repair_model\n",
    "\n",
    "repaired_model = repair_model(merged_model, models_permuted_to_universe, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(repaired_model, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for the best merge in the simplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(models_permuted_to_universe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import differential_evolution\n",
    "\n",
    "ref_model = copy.deepcopy(models_permuted_to_universe[\"a\"])\n",
    "\n",
    "\n",
    "def evaluate_model(alphas, models, loader):\n",
    "\n",
    "    params = {symb: model.model.state_dict() for symb, model in models.items()}\n",
    "\n",
    "    merged_model_params = average_models(params, reduction_fn=red_fns[\"mean\"], coeffs=alphas)\n",
    "\n",
    "    merged_model = copy.deepcopy(ref_model)\n",
    "    merged_model.model.load_state_dict(merged_model_params)\n",
    "\n",
    "    acc = trainer.test(merged_model, loader)[0][\"acc/test\"]\n",
    "\n",
    "    # penalty = 1e6 * abs(np.sum(alphas) - 1)\n",
    "\n",
    "    return -acc\n",
    "\n",
    "\n",
    "# # Constraints: alphas should sum to 1\n",
    "def constraint_eq(alphas):\n",
    "    return np.sum(alphas) - 1\n",
    "\n",
    "\n",
    "n = len(models)\n",
    "# Bounds: alphas should be between 0 and 1\n",
    "bounds = [(0, 1) for _ in range(n)]\n",
    "\n",
    "# Perform the optimization\n",
    "loss_fn = partial(evaluate_model, loader=train_loader)\n",
    "# result = minimize(loss_fn, initial_guess,\n",
    "#                   bounds=bounds)\n",
    "\n",
    "# Perform the optimization\n",
    "# result = minimize(loss_fn, initial_guess, method='SLSQP', bounds=bounds, constraints={'type': 'eq', 'fun': constraint_eq},\n",
    "#                   options={'ftol': 1e-2, 'disp': True})\n",
    "\n",
    "result = differential_evolution(\n",
    "    evaluate_model,\n",
    "    args=(models_permuted_to_universe, train_loader),\n",
    "    bounds=bounds,\n",
    "    strategy=\"best1bin\",\n",
    "    maxiter=10,\n",
    "    popsize=20,\n",
    "    tol=1e-3,\n",
    "    mutation=(0.5, 1),\n",
    "    recombination=0.7,\n",
    "    disp=True,\n",
    ")\n",
    "\n",
    "# Get the optimal alphas\n",
    "optimal_alphas = result.x\n",
    "\n",
    "print(f\"Optimal alphas: {optimal_alphas}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Optimal alphas: {optimal_alphas}\")\n",
    "\n",
    "merged_model_params = average_models(\n",
    "    {symb: model.model.state_dict() for symb, model in models_permuted_to_universe.items()},\n",
    "    reduction_fn=red_fns[\"mean\"],\n",
    "    coeffs=optimal_alphas,\n",
    ")\n",
    "\n",
    "merged_model = copy.deepcopy(ref_model)\n",
    "\n",
    "merged_model.model.load_state_dict(merged_model_params)\n",
    "\n",
    "loss = trainer.test(merged_model, loader)[0][\"loss/test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_permuted_to_universe = {symb: model.to(\"cpu\") for symb, model in models_permuted_to_universe.items()}\n",
    "fishers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccmm.matching.fisher_merging import compute_fisher_for_model\n",
    "\n",
    "num_fisher_samples = 500\n",
    "fisher_train_subset = Subset(train_dataset, list(range(num_fisher_samples)))\n",
    "\n",
    "fisher_train_loader = DataLoader(fisher_train_subset, batch_size=8)\n",
    "\n",
    "num_classes = core_cfg.dataset.num_classes\n",
    "\n",
    "fishers = {\n",
    "    symbol: compute_fisher_for_model(model.model, fisher_train_loader, num_classes)\n",
    "    for symbol, model in models_permuted_to_universe.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_permuted_to_universe.values():\n",
    "    model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = copy.deepcopy(models[\"a\"])\n",
    "for param_name, param in merged_model.model.named_parameters():\n",
    "    param.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = copy.deepcopy(models_permuted_to_universe[\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol, model in models_permuted_to_universe.items():\n",
    "    for param_name, param in model.model.named_parameters():\n",
    "\n",
    "        to_add = torch.zeros_like(param.data)\n",
    "        target_param = target_model.model.state_dict()[param_name].data\n",
    "\n",
    "        fish_coeff = fishers[symbol][param_name]\n",
    "\n",
    "        all_fishers = torch.stack([fishers[symb][param_name] for symb in symbols])\n",
    "        fish_coeff = fish_coeff / all_fishers.sum(dim=0)\n",
    "\n",
    "        tol = 1e-12\n",
    "\n",
    "        num_small_fish = (fish_coeff < tol).sum()\n",
    "        ratio_small_fish = num_small_fish / fish_coeff.numel()\n",
    "\n",
    "        pylogger.info(f\"Number of small fisher coefficients: {num_small_fish}, ratio: {ratio_small_fish}\")\n",
    "        pylogger.info(f\"Average fisher: {fish_coeff.mean()}\")\n",
    "\n",
    "        to_add[fish_coeff < tol] = target_param[fish_coeff < tol] * (1 / num_models)\n",
    "\n",
    "        to_add[fish_coeff >= tol] = param.data[fish_coeff >= tol] * (1 / num_models) * fish_coeff[fish_coeff >= tol]\n",
    "\n",
    "        merged_model.model.state_dict()[param_name].add_(to_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(merged_model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(target_model, loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repaired_model = repair_model(merged_model, models_permuted_to_universe, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.test(repaired_model, loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge in the basins of the endpoint models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each model having symbol s, try to align all the other models to it. Then, average the aligned models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {symbol: {\"vanilla\": None, \"repaired\": None} for symbol in symbols}\n",
    "\n",
    "for symbol in symbols:\n",
    "\n",
    "    # all the other models permuted to the current model\n",
    "    mapped_models = {\n",
    "        other_symb: models_permuted_pairwise[symbol][other_symb]\n",
    "        for other_symb, model in models.items()\n",
    "        if other_symb != symbol\n",
    "    }\n",
    "    mapped_params = {symb: model.model.state_dict() for symb, model in mapped_models.items()}\n",
    "    mapped_params[symbol] = models[symbol].model.state_dict()\n",
    "\n",
    "    merged_model = copy.deepcopy(models[symbol])\n",
    "\n",
    "    mean_model = average_models(mapped_params, reduction_fn=red_fns[\"mean\"])\n",
    "\n",
    "    merged_model.model.load_state_dict(mean_model)\n",
    "\n",
    "    vanilla_res = trainer.test(merged_model, loader)[0]\n",
    "\n",
    "    repaired_model = repair_model(merged_model, mapped_models, train_loader)\n",
    "\n",
    "    repair_res = trainer.test(repaired_model, loader)[0]\n",
    "\n",
    "    results[symbol][\"vanilla\"] = vanilla_res\n",
    "    results[symbol][\"repaired\"] = repair_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the models and their accuracies\n",
    "vanilla_accuracies = [results[symbol][\"vanilla\"][\"acc/test\"] for symbol in symbols]\n",
    "repaired_accuracies = [results[symbol][\"repaired\"][\"acc/test\"] for symbol in symbols]\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(symbols))\n",
    "\n",
    "# Plotting the bar chart\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars1 = ax.bar(index, vanilla_accuracies, bar_width, label=\"Vanilla\", color=palette[\"light red\"])\n",
    "bars2 = ax.bar(index + bar_width, repaired_accuracies, bar_width, label=\"Repaired\", color=palette[\"green\"])\n",
    "\n",
    "# Adding labels, title, and legend\n",
    "ax.set_xlabel(\"Basin\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Accuracy when averaging in different basins\")\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(models)\n",
    "ax.legend(loc=\"lower center\", bbox_to_anchor=(0.5, -0.3), ncol=2)\n",
    "ax.set_ylim(0, max(repaired_accuracies) + 0.1)  # Adding some space above the tallest bar\n",
    "\n",
    "# Adding the accuracy values on top of the bars\n",
    "def add_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(\n",
    "            f\"{height:.2f}\",\n",
    "            xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "            xytext=(0, 3),  # 3 points vertical offset\n",
    "            textcoords=\"offset points\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "\n",
    "add_labels(bars1)\n",
    "add_labels(bars2)\n",
    "\n",
    "plt.savefig(\"figures/accuracy_in_different_basins.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching the N-1 models in each subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Match and merge all the (n-1)-subsets of the n models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each symbol, collect the subset of the remaining n-1 models\n",
    "symbol_subsets = {symbol: set(symbols).difference(symbol) for symbol in symbols}\n",
    "\n",
    "merged_model = copy.deepcopy(models[symbols[0]])\n",
    "\n",
    "matched_subsets_results = {}\n",
    "merged_models = {}\n",
    "\n",
    "for symbol_subset in symbol_subsets.values():\n",
    "\n",
    "    combinations = get_all_symbols_combinations(symbol_subset)\n",
    "    canonical_combinations = [(source, target) for (source, target) in combinations if source < target]\n",
    "    model_subset = {symb: models[symb] for symb in symbol_subset}\n",
    "\n",
    "    perm_indices, _ = frank_wolfe_synchronized_matching(\n",
    "        models=model_subset,\n",
    "        perm_spec=permutation_spec,\n",
    "        symbols=list(symbol_subset),\n",
    "        combinations=canonical_combinations,\n",
    "        max_iter=max_iter,\n",
    "        initialization_method=initialization_method,\n",
    "        keep_soft_perms=keep_soft_perms,\n",
    "    )\n",
    "\n",
    "    pylogger.info(f\"Symbol subset: {symbol_subset}\")\n",
    "\n",
    "    models_to_univ_subset = get_models_permuted_to_univ(perm_indices, model_subset, symbol_subset, keep_soft_perms)\n",
    "\n",
    "    model_params = [model.model.state_dict() for model in models_to_univ_subset.values()]\n",
    "\n",
    "    merged_params = average_models(model_params, reduction_fn=red_fns[\"mean\"])\n",
    "    merged_model.model.load_state_dict(merged_params)\n",
    "\n",
    "    merged_results = trainer.test(merged_model, loader)[0]\n",
    "\n",
    "    repaired_model = repair_model(merged_model, models_to_univ_subset, train_loader)\n",
    "\n",
    "    repair_results = trainer.test(repaired_model, loader)[0]\n",
    "\n",
    "    matched_subsets_results[tuple(symbol_subset)] = {\"merged\": merged_results, \"repaired\": repair_results}\n",
    "    merged_models[tuple(symbol_subset)] = {\"merged\": merged_model, \"repaired\": repaired_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_subsets_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy histogram when merging matched subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = list(matched_subsets_results.keys())\n",
    "merged_accuracies = [matched_subsets_results[combo][\"merged\"][\"acc/test\"] for combo in combinations]\n",
    "repaired_accuracies = [matched_subsets_results[combo][\"repaired\"][\"acc/test\"] for combo in combinations]\n",
    "\n",
    "combination_strings = [\"(\" + \",\".join(sorted(combo)) + \")\" for combo in combinations]\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(combination_strings))\n",
    "\n",
    "# Plotting the bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "ax.set_ylim(0, max(repaired_accuracies) + 0.1)  # Adding some space above the tallest bar\n",
    "\n",
    "bars1 = ax.bar(index, merged_accuracies, bar_width, label=\"Vanilla\", color=palette[\"light red\"])\n",
    "bars2 = ax.bar(index + bar_width, repaired_accuracies, bar_width, label=\"Repaired\", color=palette[\"green\"])\n",
    "\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Accuracy when merging model subsets\")\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(combination_strings, rotation=45, ha=\"right\")\n",
    "ax.legend(loc=\"lower center\", bbox_to_anchor=(0.5, -0.3), ncol=2)\n",
    "\n",
    "# Adding the accuracy values on top of the bars\n",
    "def add_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(\n",
    "            f\"{height:.2f}\",\n",
    "            xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "            xytext=(0, 3),  # 3 points vertical offset\n",
    "            textcoords=\"offset points\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "\n",
    "add_labels(bars1)\n",
    "add_labels(bars2)\n",
    "\n",
    "plt.savefig(\"figures/accuracy_model_subsets_matched.pdf\", bbox_inches=\"tight\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching all the N models and only averaging the N-1 models in each subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_combinations = get_all_symbols_combinations(symbols)\n",
    "canonical_combinations = [(source, target) for (source, target) in all_combinations if source < target]\n",
    "\n",
    "perm_indices, _ = frank_wolfe_synchronized_matching(\n",
    "    models=models,\n",
    "    perm_spec=permutation_spec,\n",
    "    symbols=symbols,\n",
    "    combinations=canonical_combinations,\n",
    "    max_iter=max_iter,\n",
    "    initialization_method=initialization_method,\n",
    "    keep_soft_perms=keep_soft_perms,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_model = copy.deepcopy(models[symbols[0]])\n",
    "models_permuted_to_universe = get_models_permuted_to_univ(perm_indices, models, symbols, keep_soft_perms)\n",
    "\n",
    "results_norealign = {}\n",
    "merged_models_norealign = {}\n",
    "\n",
    "for symbol_subset in symbol_subsets.values():\n",
    "    pylogger.info(f\"Symbol subset: {symbol_subset}\")\n",
    "\n",
    "    models_to_univ_subset = {symb: models_permuted_to_universe[symb] for symb in symbol_subset}\n",
    "\n",
    "    model_params = [model.model.state_dict() for model in models_to_univ_subset.values()]\n",
    "\n",
    "    merged_params = average_models(model_params, reduction_fn=red_fns[\"mean\"])\n",
    "    merged_model.model.load_state_dict(merged_params)\n",
    "\n",
    "    merged_results = trainer.test(merged_model, loader)[0]\n",
    "\n",
    "    repaired_model = repair_model(merged_model, models_to_univ_subset, train_loader)\n",
    "\n",
    "    repair_results = trainer.test(repaired_model, loader)[0]\n",
    "\n",
    "    results_norealign[tuple(symbol_subset)] = {\"merged\": merged_results, \"repaired\": repair_results}\n",
    "    merged_models_norealign[tuple(symbol_subset)] = {\"merged\": merged_model, \"repaired\": repaired_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_norealign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy histogram when merging subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combinations = list(results_norealign.keys())\n",
    "merged_accuracies = [results_norealign[combo][\"merged\"][\"acc/test\"] for combo in combinations]\n",
    "repaired_accuracies = [results_norealign[combo][\"repaired\"][\"acc/test\"] for combo in combinations]\n",
    "\n",
    "combination_strings = [\"(\" + \",\".join(sorted(combo)) + \")\" for combo in combinations]\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(combination_strings))\n",
    "\n",
    "# Plotting the bar chart\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "bars1 = ax.bar(index, merged_accuracies, bar_width, label=\"Vanilla\", color=palette[\"light red\"])\n",
    "bars2 = ax.bar(index + bar_width, repaired_accuracies, bar_width, label=\"Repaired\", color=palette[\"green\"])\n",
    "\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"Accuracy when merging model subsets\")\n",
    "\n",
    "ax.set_ylim(0, max(repaired_accuracies) + 0.1)  # Adding some space above the tallest bar\n",
    "\n",
    "ax.set_xticks(index + bar_width / 2)\n",
    "ax.set_xticklabels(combination_strings, rotation=45, ha=\"right\")\n",
    "ax.legend(loc=\"lower center\", bbox_to_anchor=(0.5, -0.3), ncol=2)\n",
    "\n",
    "# Adding the accuracy values on top of the bars\n",
    "def add_labels(bars):\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(\n",
    "            f\"{height:.2f}\",\n",
    "            xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "            xytext=(0, 3),  # 3 points vertical offset\n",
    "            textcoords=\"offset points\",\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "        )\n",
    "\n",
    "\n",
    "add_labels(bars1)\n",
    "add_labels(bars2)\n",
    "\n",
    "plt.savefig(\"figures/accuracy_model_subsets.pdf\", bbox_inches=\"tight\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
