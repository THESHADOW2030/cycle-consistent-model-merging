{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict\n",
    "\n",
    "import hydra\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import omegaconf\n",
    "import seaborn as sns\n",
    "import torch  # noqa\n",
    "import wandb\n",
    "from hydra.utils import instantiate\n",
    "from matplotlib import tri\n",
    "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
    "from omegaconf import DictConfig\n",
    "from pytorch_lightning import LightningModule\n",
    "from scipy.stats import qmc\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from ccmm.matching.utils import perm_indices_to_perm_matrix\n",
    "\n",
    "from nn_core.callbacks import NNTemplateCore\n",
    "from nn_core.common import PROJECT_ROOT\n",
    "from nn_core.common.utils import seed_index_everything\n",
    "from nn_core.model_logging import NNLogger\n",
    "\n",
    "from torch.utils.data import DataLoader, Subset, SubsetRandomSampler\n",
    "\n",
    "import ccmm  # noqa\n",
    "from ccmm.matching.utils import (\n",
    "    apply_permutation_to_statedict,\n",
    "    get_all_symbols_combinations,\n",
    "    plot_permutation_history_animation,\n",
    "    restore_original_weights,\n",
    ")\n",
    "from ccmm.utils.utils import (\n",
    "    linear_interpolation,\n",
    "    load_model_from_info,\n",
    "    load_permutations,\n",
    "    map_model_seed_to_symbol,\n",
    "    save_factored_permutations,\n",
    ")\n",
    "\n",
    "from ccmm.utils.utils import vector_to_state_dict\n",
    "import pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams[\"font.family\"] = \"serif\"\n",
    "sns.set_context(\"talk\")\n",
    "matplotlib.rcParams[\"text.usetex\"] = True\n",
    "\n",
    "logging.getLogger(\"lightning.pytorch\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"torch\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"pytorch_lightning.accelerators.cuda\").setLevel(logging.WARNING)\n",
    "pylogger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "from typing import Dict, List\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=str(\"../conf\"), job_name=\"matching_n_models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = compose(config_name=\"matching_n_models\", overrides=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core_cfg = cfg  # NOQA\n",
    "cfg = cfg.matching\n",
    "\n",
    "seed_index_everything(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sampled_points = 100  # 2048\n",
    "num_test_samples = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = instantiate(core_cfg.dataset.test.transform)\n",
    "\n",
    "train_dataset = instantiate(core_cfg.dataset.train, transform=transform)\n",
    "test_dataset = instantiate(core_cfg.dataset.test, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.batch_size, num_workers=cfg.num_workers)\n",
    "\n",
    "test_subset = Subset(test_dataset, list(range(num_test_samples)))\n",
    "\n",
    "test_loader = DataLoader(test_subset, batch_size=1000, num_workers=cfg.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = instantiate(cfg.trainer, enable_progress_bar=False, enable_model_summary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {a: 1, b: 2, c: 3, ..}\n",
    "symbols_to_seed: Dict[int, str] = {map_model_seed_to_symbol(seed): seed for seed in cfg.model_seeds}\n",
    "\n",
    "models: Dict[str, LightningModule] = {\n",
    "    map_model_seed_to_symbol(seed): load_model_from_info(cfg.model_info_path, seed) for seed in cfg.model_seeds\n",
    "}\n",
    "\n",
    "pylogger.info(f\"Using model {core_cfg.model.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# always permute the model having larger character order, i.e. c -> b, b -> a and so on ...\n",
    "symbols = set(symbols_to_seed.keys())\n",
    "sorted_symbols = sorted(symbols, reverse=False)\n",
    "\n",
    "# (a, b), (a, c), (b, c), ...\n",
    "all_combinations = get_all_symbols_combinations(symbols)\n",
    "# combinations of the form (a, b), (a, c), (b, c), .. and not (b, a), (c, a) etc\n",
    "canonical_combinations = [(source, target) for (source, target) in all_combinations if source < target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylogger.info(f\"Matching the following model pairs: {canonical_combinations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load permutation specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_spec_builder = instantiate(core_cfg.model.permutation_spec_builder)\n",
    "permutation_spec = permutation_spec_builder.create_permutation()\n",
    "\n",
    "ref_model = list(models.values())[0]\n",
    "assert set(permutation_spec.layer_and_axes_to_perm.keys()) == set(ref_model.model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = instantiate(cfg.matcher, permutation_spec=permutation_spec)\n",
    "pylogger.info(f\"Matcher: {matcher.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations, perm_history = matcher(models, symbols=sorted_symbols, combinations=canonical_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {symb: model.to(\"cpu\") for symb, model in models.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permute models to universe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_permuted_to_universe = {symbol: copy.deepcopy(model) for symbol, model in models.items()}\n",
    "\n",
    "for symbol, model in models_permuted_to_universe.items():\n",
    "    permuted_params = apply_permutation_to_statedict(permutation_spec, permutations[symbol], model.model.state_dict())\n",
    "    models_permuted_to_universe[symbol].model.load_state_dict(permuted_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol, model in models_permuted_to_universe.items():\n",
    "    trainer.test(models_permuted_to_universe[symbol], test_loader)\n",
    "    trainer.test(models[symbol], test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample points in the param space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundaries = [[-0.5], [0.5]]\n",
    "lower_bounds = np.array([boundaries[0][0], boundaries[0][0]])\n",
    "upper_bounds = np.array([boundaries[1][0], boundaries[1][0]])\n",
    "\n",
    "pylogger.info(f\"Lower bounds: {lower_bounds}\")\n",
    "pylogger.info(f\"Upper bounds: {upper_bounds}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_points_plane = qmc.scale(\n",
    "    qmc.Sobol(d=2, scramble=True, seed=cfg.seed_index).random(num_sampled_points),\n",
    "    [-0.5, -0.5],\n",
    "    [0.5, 0.5],\n",
    ")\n",
    "\n",
    "pylogger.info(random_points_plane[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_models = {symbol: torch.nn.utils.parameters_to_vector(model.parameters()) for symbol, model in models.items()}\n",
    "flat_models_permuted_to_universe = {\n",
    "    symbol: torch.nn.utils.parameters_to_vector(model.parameters())\n",
    "    for symbol, model in models_permuted_to_universe.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol, model in models.items():\n",
    "    norm = torch.norm(flat_models[symbol])\n",
    "    pylogger.info(f\"Norm of {symbol}: {norm}\")\n",
    "    pylogger.info(f\"Norm of {symbol} permuted: {torch.norm(flat_models_permuted_to_universe[symbol])}\")\n",
    "\n",
    "    norm_diff = torch.norm(flat_models[symbol] - flat_models_permuted_to_universe[symbol])\n",
    "    pylogger.info(f\"Norm diff of {symbol}: {norm_diff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix of the cosine products\n",
    "cosine_matrix = np.zeros((len(models), len(models)))\n",
    "\n",
    "for i, (symbol_i, model_i) in enumerate(models.items()):\n",
    "    for j, (symbol_j, model_j) in enumerate(models.items()):\n",
    "        cosine_matrix[i, j] = flat_models_permuted_to_universe[symbol_i].dot(flat_models[symbol_j]) / (\n",
    "            torch.norm(flat_models_permuted_to_universe[symbol_i]) * torch.norm(flat_models[symbol_j])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Barycentric coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def get_pentagon_vertices(center_x, center_y, radius):\n",
    "    \"\"\"\n",
    "    Get the vertices of a pentagon centered at (center_x, center_y) with the given radius.\n",
    "    \"\"\"\n",
    "    pentagon_vertices = []\n",
    "\n",
    "    for i in range(5):\n",
    "\n",
    "        angle_deg = 72 * i  # 72 degrees between each point\n",
    "        angle_rad = math.radians(angle_deg)  # Convert to radians\n",
    "\n",
    "        x = radius * math.cos(angle_rad) + center_x\n",
    "        y = radius * math.sin(angle_rad) + center_y\n",
    "\n",
    "        pentagon_vertices.append((x, y))\n",
    "\n",
    "    return np.array(pentagon_vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_barycentric_coordinates(x):\n",
    "    \"\"\"\n",
    "    x: point in the plane (2, )\n",
    "    \"\"\"\n",
    "    origins = get_pentagon_vertices(0.0, 0.0, 0.45)\n",
    "\n",
    "    # (2, num_models)\n",
    "    A = origins.transpose(1, 0)\n",
    "\n",
    "    # (3, num_models)\n",
    "    A = np.vstack([A, np.ones(5)])\n",
    "\n",
    "    # (3, )\n",
    "    x = np.append(x, 1)\n",
    "\n",
    "    z, residuals, rank, s = np.linalg.lstsq(A, x, rcond=None)\n",
    "\n",
    "    z -= z.min()\n",
    "    z /= z.sum()\n",
    "\n",
    "    A = torch.from_numpy(A)\n",
    "    z = torch.from_numpy(z)\n",
    "    x = torch.from_numpy(x)\n",
    "    # assert torch.allclose(A @ z, x)\n",
    "\n",
    "    return z.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = get_pentagon_vertices(0.0, 0.0, 0.45)\n",
    "\n",
    "# (2, num_models)\n",
    "A = origins.transpose(1, 0)\n",
    "\n",
    "# (3, num_models)\n",
    "A = np.vstack([A, np.ones(5)])\n",
    "\n",
    "x = origins[0]\n",
    "x = np.append(x, 1)\n",
    "\n",
    "z, residuals, rank, s = np.linalg.lstsq(A, x, rcond=None)\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# origins = get_pentagon_vertices(0.0, 0.0, 0.9)\n",
    "# A = origins.transpose(1, 0)\n",
    "\n",
    "# A.shape\n",
    "# # # plot the origins\n",
    "# # plt.scatter(A[0], A[1], c='black', s=100)\n",
    "# # plt.axis('equal')\n",
    "# # # plot the sphere with radius = 0.9\n",
    "# # circle = plt.Circle((0, 0), 0.9, color='black', fill=False)\n",
    "\n",
    "# # plt.gca().add_patch(circle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Represent permuted models as barycentric coordinates wrt the n models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def represent_wrt_models(model_to_repr, flat_models):\n",
    "\n",
    "    # (num_params_per_model, num_models)\n",
    "    A = torch.stack(list(flat_models.values()), dim=1)\n",
    "\n",
    "    scaling = 1\n",
    "    # Augment A with an additional row for the sum-to-one constraint\n",
    "    ones_row = torch.ones(1, A.shape[1]) * scaling\n",
    "\n",
    "    # (num_params_per_model + 1, num_models)\n",
    "\n",
    "    A_augmented = torch.cat([A, ones_row], dim=0)\n",
    "\n",
    "    # Augment the target model with an additional element for the sum-to-one constraint\n",
    "    # (num_params_per_model + 1,)\n",
    "    target_augmented = torch.cat([model_to_repr, torch.tensor([scaling])])\n",
    "\n",
    "    # Solve the linear system (least squares)\n",
    "    # want z such that Az = x\n",
    "    # x is the target model\n",
    "    barycentric_coords = torch.linalg.lstsq(A_augmented, target_augmented.unsqueeze(1)).solution\n",
    "\n",
    "    return barycentric_coords.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "origins = get_pentagon_vertices(0.0, 0.0, 0.45)\n",
    "\n",
    "model_2D_repr = {symbol: None for symbol in symbols_to_seed.keys()}\n",
    "universe_model_2D_repr = {symbol: None for symbol in symbols_to_seed.keys()}\n",
    "for model_num, (symbol, perm_model) in enumerate(flat_models_permuted_to_universe.items()):\n",
    "\n",
    "    model_baryc_coordinates = represent_wrt_models(perm_model, flat_models)\n",
    "\n",
    "    model_baryc_coordinates = model_baryc_coordinates / model_baryc_coordinates.sum(axis=0)\n",
    "\n",
    "    model_2D_repr[symbol] = origins[model_num]\n",
    "    universe_model_2D_repr[symbol] = (model_baryc_coordinates * origins).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also solve for the barycentric coordinates of the models in the high-dimensional space\n",
    "# scale the entire equation constraining the coefficients to sum up to 1 by a large scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def evaluate_model_interp_on_point(point, flat_models, model, trainer, test_loader):\n",
    "\n",
    "    # (num_models, )\n",
    "    baryc_coords = represent_barycentric_coordinates(point).unsqueeze(1)\n",
    "\n",
    "    # (num_models, num_params_per_model)\n",
    "    flat_models = torch.stack(list(flat_models.values()))\n",
    "\n",
    "    new_flat_params = (flat_models * baryc_coords).sum(dim=0)\n",
    "\n",
    "    new_params = vector_to_state_dict(new_flat_params, model.model)\n",
    "\n",
    "    model.model.load_state_dict(new_params)\n",
    "\n",
    "    eval_results = trainer.test(model, test_loader, verbose=False)\n",
    "\n",
    "    return eval_results\n",
    "\n",
    "\n",
    "# random_points_plane = np.concatenate((origins, random_points_plane))\n",
    "\n",
    "model = copy.deepcopy(models[\"a\"])\n",
    "eval_results = np.array(\n",
    "    [\n",
    "        evaluate_model_interp_on_point(point, flat_models, model, trainer, test_loader)\n",
    "        for point in tqdm(random_points_plane)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = np.array([res[0][\"loss/test\"] for res in eval_results])\n",
    "test_losses[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference models as basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# proj = lambda a, b: torch.dot(a, b) / torch.dot(b, b) * b\n",
    "# norm = lambda a: torch.sqrt(torch.dot(a, a))\n",
    "# normalize = lambda a: a / norm(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_basis_vectors(origin_model, basis_model_1, basis_model_2):\n",
    "#     basis1 = basis_model_1 - origin_model\n",
    "#     scale1 = norm(basis1)\n",
    "#     basis1_normed = normalize(basis1)\n",
    "\n",
    "#     basis2 = basis_model_2 - origin_model\n",
    "#     scale2 = norm(basis2)\n",
    "#     basis2 = basis2 - proj(basis2, basis1_normed)\n",
    "#     basis2_normed = normalize(basis2)\n",
    "\n",
    "#     return basis1_normed, basis2_normed, scale1, scale2\n",
    "\n",
    "# basis_model_1, basis_model_2, scale_1, scale_2 = get_basis_vectors(origin_model=flat_models['a'], basis_model_1=flat_models['b'], basis_model_2=flat_models['c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from multiprocessing import Pool\n",
    "\n",
    "# def evaluate_model_interp_on_point(point, basis_model_1, basis_model_2, origin_model, ref_model, trainer, test_loader, scale_1, scale_2):\n",
    "\n",
    "#     # (num_models, )\n",
    "#     new_flat_params = origin_model + (scale_1 * basis_model_1 * point[0] + scale_2 * basis_model_2 * point[1])\n",
    "\n",
    "#     new_params = vector_to_state_dict(new_flat_params, ref_model.model)\n",
    "\n",
    "#     ref_model.model.load_state_dict(new_params)\n",
    "\n",
    "#     eval_results = trainer.test(ref_model, test_loader, verbose=False)\n",
    "\n",
    "#     return eval_results\n",
    "\n",
    "# ref_model = copy.deepcopy(models['a'])\n",
    "# origin_model = flat_models['a']\n",
    "\n",
    "# eval_results = np.array([evaluate_model_interp_on_point(point, scale_1=scale_1, scale_2=scale_2, basis_model_1=basis_model_1, basis_model_2=basis_model_2, origin_model=origin_model, ref_model=ref_model, trainer=trainer, test_loader=test_loader) for point in tqdm(random_points_plane)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from functools import partial\n",
    "\n",
    "# pool = Pool() #defaults to number of available CPU's\n",
    "\n",
    "\n",
    "# eval_func = partial(evaluate_model_interp_on_point, basis_model_1=basis_model_1, basis_model_2=basis_model_2, origin_model=origin_model, ref_model=ref_model, trainer=trainer, test_loader=test_loader)\n",
    "\n",
    "# results = np.zeros(len(random_points_plane))\n",
    "# for ind, res in enumerate(tqdm(pool.imap(eval_func, iter(random_points_plane)), total=len(random_points_plane))):\n",
    "#     results[ind] = res\n",
    "\n",
    "# # eval_results = np.array([evaluate_model_interp_on_point(point, basis_model_1=basis_model_1, basis_model_2=basis_model_2, origin_model=origin_model, ref_model=ref_model, trainer=trainer, test_loader=test_loader) for point in tqdm(random_points_plane)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_losses = np.array([res[0]['loss/test'] for res in eval_results])\n",
    "# test_losses[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Represent models 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def represent_wrt_models(model_to_repr, origin_model, basis1, basis2, scale_1, scale_2):\n",
    "\n",
    "#     x_coord = torch.dot(model_to_repr - origin_model, basis1) / scale_1\n",
    "#     y_coord = torch.dot(model_to_repr - origin_model, basis2) / scale_2\n",
    "\n",
    "#     return torch.stack([x_coord, y_coord]).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_2D_repr = {symbol: None for symbol in symbols_to_seed.keys()}\n",
    "# universe_model_2D_repr = {symbol: None for symbol in symbols_to_seed.keys()}\n",
    "# for model_num, (symbol, perm_model) in enumerate(flat_models_permuted_to_universe.items()):\n",
    "\n",
    "#     model_2D = represent_wrt_models(flat_models[symbol], origin_model=flat_models['a'], basis1= basis_model_1, basis2=basis_model_2, scale_1=scale_1, scale_2=scale_2)\n",
    "#     model_2D_perm = represent_wrt_models(perm_model, origin_model=flat_models['a'], basis1= basis_model_1, basis2=basis_model_2, scale_1=scale_1, scale_2=scale_2)\n",
    "\n",
    "#     model_2D_repr[symbol] = model_2D\n",
    "#     universe_model_2D_repr[symbol] = model_2D_perm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylogger.info(model_2D_repr)\n",
    "pylogger.info(universe_model_2D_repr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create grid values first.\n",
    "xi = np.linspace(boundaries[0][0], boundaries[1][0])\n",
    "yi = np.linspace(boundaries[0][0], boundaries[1][0])\n",
    "\n",
    "# Linearly interpolate the data (x, y) on a grid defined by (xi, yi).\n",
    "triang = tri.Triangulation(random_points_plane[:, 0], random_points_plane[:, 1])\n",
    "\n",
    "# We need to cap the maximum loss value so that the contouring is not completely saturated by wildly large losses\n",
    "interpolator = tri.LinearTriInterpolator(triang, np.clip(test_losses, None, 5))\n",
    "\n",
    "# interpolator = tri.LinearTriInterpolator(triang, jnp.log(jnp.minimum(1.5, eval_results[:, 0])))\n",
    "zi = interpolator(*np.meshgrid(xi, yi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_name = \"coolwarm_r\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_colormap(cmap, minval=0.0, maxval=1.0, n=100):\n",
    "    return colors.LinearSegmentedColormap.from_list(\n",
    "        \"trunc({n},{a:.2f},{b:.2f})\".format(n=cmap.name, a=minval, b=maxval),\n",
    "        cmap(np.linspace(minval, maxval, n)),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "num_levels = 13\n",
    "\n",
    "plt.contour(xi, yi, zi, levels=num_levels, linewidths=0.25, colors=\"grey\", alpha=0.5)\n",
    "\n",
    "# cmap = truncate_colormap(plt.get_cmap(cmap_name), 0.0, 1)\n",
    "\n",
    "plt.contourf(xi, yi, zi, levels=num_levels, cmap=plt.get_cmap(cmap_name), extend=\"both\")\n",
    "plt.colorbar()\n",
    "\n",
    "label_bboxes = dict(facecolor=\"tab:grey\", boxstyle=\"round\", edgecolor=\"none\", alpha=0.5)\n",
    "\n",
    "for symbol, point in model_2D_repr.items():\n",
    "    plt.scatter(point[0], point[1], marker=\"x\", color=\"black\", zorder=10)\n",
    "    plt.text(\n",
    "        point[0] - 0.075,\n",
    "        point[1] + 0.1,\n",
    "        r\"${\\bf \\Theta_\" + symbol + r\"}$\",\n",
    "        color=\"white\",\n",
    "        fontsize=24,\n",
    "        bbox=label_bboxes,\n",
    "        horizontalalignment=\"right\",\n",
    "        verticalalignment=\"top\",\n",
    "    )\n",
    "\n",
    "    universe_point = universe_model_2D_repr[symbol]\n",
    "    plt.scatter(universe_point[0], universe_point[1], marker=\"o\", color=\"black\", zorder=10)\n",
    "\n",
    "    connectionstyle = \"arc3,rad=-0.3\"\n",
    "    plt.annotate(\n",
    "        \"\",\n",
    "        xy=(point[0], point[1]),\n",
    "        xytext=(universe_point[0], universe_point[1]),\n",
    "        arrowprops=dict(\n",
    "            arrowstyle=\"-\",\n",
    "            edgecolor=\"black\",\n",
    "            facecolor=\"none\",\n",
    "            linewidth=5,\n",
    "            linestyle=(0, (5, 3)),\n",
    "            shrinkA=20,\n",
    "            shrinkB=15,\n",
    "            connectionstyle=connectionstyle,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Draw arrow head only\n",
    "    plt.annotate(\n",
    "        \"\",\n",
    "        xy=(point[0], point[1]),\n",
    "        xytext=(universe_point[0], universe_point[1]),\n",
    "        arrowprops=dict(\n",
    "            arrowstyle=\"<|-\",\n",
    "            edgecolor=\"none\",\n",
    "            facecolor=\"black\",\n",
    "            mutation_scale=40,\n",
    "            linewidth=0,\n",
    "            shrinkA=12.5,\n",
    "            shrinkB=15,\n",
    "            connectionstyle=connectionstyle,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "# mean_ae = flat_models['a'] / 2 + flat_models['e'] / 2\n",
    "# mean_ae_2D = represent_wrt_models(mean_ae, origin_model=flat_models['a'], basis1= basis_model_1, basis2=basis_model_2, scale=scale)\n",
    "# plt.scatter(mean_ae_2D[0], mean_ae_2D[1], marker=\"o\", color=\"yellow\", zorder=10)\n",
    "\n",
    "\n",
    "# mean_ab = flat_models['a'] / 2 + flat_models['c'] / 2\n",
    "# mean_ab_2D = represent_wrt_models(mean_ab, origin_model=flat_models['a'], basis1= basis_model_1, basis2=basis_model_2, scale=scale)\n",
    "# plt.scatter(mean_ab_2D[0], mean_ab_2D[1], marker=\"o\", color=\"yellow\", zorder=10)\n",
    "\n",
    "# mean_bc = flat_models['b'] / 2 + flat_models['c'] / 2\n",
    "# mean_bc_2D = represent_wrt_models(mean_bc, origin_model=flat_models['a'], basis1= basis_model_1, basis2=basis_model_2, scale=scale)\n",
    "# plt.scatter(mean_bc_2D[0], mean_bc_2D[1], marker=\"o\", color=\"yellow\", zorder=10)\n",
    "\n",
    "# mean_abc = flat_models['a'] / 3 + flat_models['b'] / 3 + flat_models['c'] / 3\n",
    "# for interp_par in interp_params:\n",
    "#     interp_par_2D = represent_wrt_models(interp_par, origin_model=flat_models['a'], basis1= basis_model_1, basis2=basis_model_2, scale=scale)\n",
    "#     plt.scatter(interp_par_2D[0], interp_par_2D[1], marker=\"o\", color=\"yellow\", zorder=10)\n",
    "# mean_abc_2D = represent_wrt_models(mean_abc, origin_model=flat_models['a'], basis1= basis_model_1, basis2=basis_model_2, scale=scale)\n",
    "# plt.scatter(mean_abc_2D[0], mean_abc_2D[1], marker=\"o\", color=\"yellow\", zorder=10)\n",
    "\n",
    "box_x = 0.5\n",
    "box_y = 1.5\n",
    "title_text = r\"$C^2M^2$\"\n",
    "\n",
    "# Draw box only\n",
    "plt.text(\n",
    "    box_x,\n",
    "    box_y,\n",
    "    title_text,\n",
    "    color=(0.0, 0.0, 0.0, 0.0),\n",
    "    fontsize=24,\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    "    bbox=dict(boxstyle=\"round\", fc=(1, 1, 1, 1), ec=\"black\", pad=0.4),\n",
    ")\n",
    "# Draw text only\n",
    "plt.text(\n",
    "    box_x,\n",
    "    box_y - 0.0115,\n",
    "    title_text,\n",
    "    color=(0.0, 0.0, 0.0, 1.0),\n",
    "    fontsize=24,\n",
    "    horizontalalignment=\"center\",\n",
    "    verticalalignment=\"center\",\n",
    ")\n",
    "\n",
    "\n",
    "# plt.colorbar()\n",
    "plt.xlim(-0.4, 1.4)\n",
    "plt.ylim(-0.45, 1.3)\n",
    "#   plt.xlim(-0.9, 1.9)\n",
    "#   plt.ylim(-0.9, 1.9)\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "plt.axis(\"equal\")\n",
    "# plt.tight_layout()\n",
    "plt.savefig(\"resnet_cifar_loss_contour.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas = np.linspace(0, 1, 25)\n",
    "\n",
    "interp_results = []\n",
    "interp_params = []\n",
    "\n",
    "model_a_2D = represent_wrt_models(\n",
    "    flat_models[\"a\"],\n",
    "    origin_model=flat_models[\"a\"],\n",
    "    basis1=basis_model_1,\n",
    "    basis2=basis_model_2,\n",
    "    scale_1=scale_1,\n",
    "    scale_2=scale_2,\n",
    ")\n",
    "model_e_2D = represent_wrt_models(\n",
    "    flat_models[\"e\"],\n",
    "    origin_model=flat_models[\"a\"],\n",
    "    basis1=basis_model_1,\n",
    "    basis2=basis_model_2,\n",
    "    scale_1=scale_1,\n",
    "    scale_2=scale_2,\n",
    ")\n",
    "\n",
    "norms = []\n",
    "results = {\"2D_interp\": [], \"N_interp\": []}\n",
    "for lamb in lambdas:\n",
    "    interp_model = flat_models[\"a\"] * lamb + flat_models[\"e\"] * (1 - lamb)\n",
    "    interp_params.append(interp_model)\n",
    "\n",
    "    new_params = vector_to_state_dict(interp_model, ref_model.model)\n",
    "    ref_model.model.load_state_dict(new_params)\n",
    "    res = trainer.test(ref_model, test_loader, verbose=False)[0][\"loss/test\"]\n",
    "\n",
    "    results[\"2D_interp\"].append(res)\n",
    "    interp_point = model_a_2D * lamb + model_e_2D * (1 - lamb)\n",
    "    # new_params_reconstructed = origin_model + (scale_1 * basis_model_1 * interp_point[0] + scale_2 * basis_model_2 * interp_point[1])\n",
    "    new_params_reconstructed = origin_model + (\n",
    "        scale_1 * basis_model_1 * interp_point[0] + scale_2 * basis_model_2 * interp_point[1]\n",
    "    )\n",
    "\n",
    "    ref_model.model.load_state_dict(vector_to_state_dict(new_params_reconstructed, ref_model.model))\n",
    "    res = trainer.test(ref_model, test_loader, verbose=False)[0][\"loss/test\"]\n",
    "    results[\"N_interp\"].append(res)\n",
    "\n",
    "    norms.append(torch.norm(new_params_reconstructed - interp_model).detach().cpu().numpy())\n",
    "\n",
    "    # interp_results.append(trainer.test(ref_model, test_loader, verbose=False))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lambdas, norms, marker=\"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[\"2D_interp\"] = np.array(results[\"2D_interp\"])\n",
    "results[\"N_interp\"] = np.array(results[\"N_interp\"])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lambdas, results[\"2D_interp\"], marker=\"o\")\n",
    "plt.plot(lambdas, results[\"N_interp\"], marker=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(flat_models[\"c\"] / norm(flat_models[\"c\"])) @ (flat_models[\"b\"] / norm(flat_models[\"b\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pylogger.info(model_a_2D)\n",
    "pylogger.info(model_c_2D)\n",
    "\n",
    "interps_on_plane = []\n",
    "for lamb in lambdas:\n",
    "    interps_on_plane.append(model_a_2D * lamb + model_c_2D * (1 - lamb))\n",
    "\n",
    "    model_interp = flat_models[\"a\"] * (1 - lamb) + flat_models[\"c\"] * lamb\n",
    "\n",
    "\n",
    "interps_on_plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind, (par, lambd) in zip(interp_params, lambdas):\n",
    "\n",
    "    interp_par_2D = represent_wrt_models(\n",
    "        par, origin_model=flat_models[\"a\"], basis1=basis_model_1, basis2=basis_model_2, scale=scale\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis_model_1 @ basis_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = np.array([res[0][\"loss/test\"] for res in interp_results])\n",
    "test_losses\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(lambdas, test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for symb, model_repr in model_2D_repr.items():\n",
    "#     print(f'{symb}: {model_repr}')\n",
    "\n",
    "for symb, model_repr in universe_model_2D_repr.items():\n",
    "    print(f\"{symb}: {model_repr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_params = vector_to_state_dict(mean_abc, ref_model.model)\n",
    "\n",
    "ref_model.model.load_state_dict(new_params)\n",
    "\n",
    "eval_results = trainer.test(ref_model, test_loader, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results[0][\"loss/test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccmm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
