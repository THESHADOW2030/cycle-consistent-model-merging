dataset_name: cifar100_tasks # cifar100

data_path: ${oc.env:PROJECT_ROOT}/data/cifar100_tasks/same_classes_disj_samples/partition-1
output_path: ${oc.env:PROJECT_ROOT}/output

data:
  _target_: ccmm.data.task_datamodule.SameClassesDisjSamplesDatamodule
  data_path: ${nn.data_path}
  only_use_sample_num: -1 # -1 means use all samples

  gpus: ${train.trainer.gpus}

  num_workers:
    train: 8
    val: 8
    test: 8

  batch_size:
    train: 100
    val: 100
    test: 100

module:
  _target_: ccmm.pl_modules.pl_module.MyLightningModule
  model_name: ResNet_${nn.module.model.depth}_${nn.module.model.widen_factor}

  model:
    _target_: ccmm.models.resnet.ResNet
    depth: 22 # 22 or 50
    num_classes: ${dataset.num_classes}
    widen_factor: 2 # 2, 4, 16, 32 by git re-basin
    norm_layer: ln

  optimizer:
    _target_: torch.optim.SGD
    lr: 0.1
    momentum: 0.9
    weight_decay: 1e-4 # checked in git-rebasin-s code (previously was 0.0005 for some reason)


  lr_scheduler:
    _target_: pl_bolts.optimizers.lr_scheduler.LinearWarmupCosineAnnealingLR
    warmup_epochs: 5 # they claim 1 warm up epoch on the paper, but I found 5 in their code
    max_epochs: ${train.trainer.max_epochs}
    warmup_start_lr: 1e-6
    eta_min: 0.0
    last_epoch: -1
